{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 â€“ Memory & LCEL Basics\n",
        "\n",
        "**Learning Goals:**\n",
        "- Understand conversational memory in LangChain\n",
        "- Compare memory types: Buffer vs Summary\n",
        "- Master LCEL (LangChain Expression Language) composition\n",
        "- Build streaming, retry, and fallback patterns\n",
        "\n",
        "**What we'll cover:**\n",
        "1. **Memory 101** - Buffer and Summary memory patterns\n",
        "2. **Memory in Chains** - Inject memory into conversational flows\n",
        "3. **LCEL Basics** - Compose runnables with `|` operator\n",
        "4. **Advanced LCEL** - Streaming, retry, fallbacks\n",
        "5. **Tool Use (Optional)** - Simple tool integration\n",
        "\n",
        "**Note:** This notebook focuses on fundamentals, not RAG. No ChromaDB or retrieval here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Config loaded:\n",
            "  LLM: openrouter (openai/gpt-4o-mini)\n",
            "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
            "  Temperature: 0.2\n",
            "  Artifacts: ./artifacts\n",
            "  Note: Temperature is 0.2 (good for conversational demos)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/ðŸš§ AI Engineer Essentials/Codes/RAG Systems/src/services/llm_services.py:338: UserWarning: âš ï¸  GROQ_API_KEY not found in environment\n",
            "  warnings.warn(f\"âš ï¸  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/ðŸš§ AI Engineer Essentials/Codes/RAG Systems/src/services/llm_services.py:338: UserWarning: âš ï¸  GOOGLE_API_KEY not found in environment\n",
            "  warnings.warn(f\"âš ï¸  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/ðŸš§ AI Engineer Essentials/Codes/RAG Systems/src/services/llm_services.py:338: UserWarning: âš ï¸  COHERE_API_KEY not found in environment\n",
            "  warnings.warn(f\"âš ï¸  {key} not found in environment\")\n"
          ]
        }
      ],
      "source": [
        "# âš™ï¸ Global Config & Services (using centralized modules)\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add parent directory to path and change to project root\n",
        "import os\n",
        "\n",
        "# Get the notebook's current directory and find project root\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == \"notebooks\":\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "# Change to project root and add to path\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"ðŸ“‚ Working directory: {os.getcwd()}\")\n",
        "\n",
        "from src.services.llm_services import (\n",
        "    load_config,\n",
        "    get_llm,\n",
        "    validate_api_keys,\n",
        "    print_config_summary\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Load configuration from config.yaml (now we're in project root)\n",
        "config = load_config(\"src/config/config.yaml\")\n",
        "\n",
        "# Validate API keys\n",
        "validate_api_keys(config, verbose=True)\n",
        "\n",
        "# Print summary\n",
        "print_config_summary(config)\n",
        "print(f\"  Note: Temperature is {config['temperature']} (good for conversational demos)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LLM initialized: openrouter / gpt-4o-mini\n",
            "\n",
            "ðŸ” Testing API connection...\n",
            "âœ… API key verified: API working!\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM using factory from llm_services\n",
        "llm = get_llm(config)\n",
        "print(f\"âœ… LLM initialized: {config['llm_provider']} / {config['llm_model']}\")\n",
        "\n",
        "# Verify API key with test completion\n",
        "print(\"\\nðŸ” Testing API connection...\")\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
        "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
        "    print(f\"âœ… API key verified: {test_msg[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ API key test failed: {e}\")\n",
        "    print(\"âš ï¸  Please check your .env file and API key configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section A: Memory 101\n",
        "\n",
        "LangChain provides memory primitives to maintain conversational context across turns.\n",
        "\n",
        "### 1. ConversationBufferMemory\n",
        "\n",
        "Stores **full chat history** in memory. Simple but can grow large.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ Buffer Memory:\n",
            "{'history': [HumanMessage(content='Hi, my name is Alice.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Alice! Nice to meet you.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's the capital of France?\", additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={})]}\n",
            "\n",
            "ðŸ’¾ Memory keys: ['history']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b5/fr4bwywx1hl34z7s66ljm40r0000gn/T/ipykernel_5324/1920384859.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  buffer_memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "### START CODE HERE ### (â‰ˆ 18-20 lines)\n",
        "# YOUR CODE HERE\n",
        "# HINTS:\n",
        "# 1. Create a ConversationBufferMemory instance with return_messages=True:\n",
        "#    buffer_memory = ConversationBufferMemory(return_messages=True)\n",
        "#\n",
        "# 2. Simulate 3 conversation turns using buffer_memory.save_context():\n",
        "#    Turn 1:\n",
        "#      - input: {\"input\": \"Hi, my name is Alice.\"}\n",
        "#      - output: {\"output\": \"Hello Alice! Nice to meet you.\"}\n",
        "#    Turn 2:\n",
        "#      - input: {\"input\": \"What's my name?\"}\n",
        "#      - output: {\"output\": \"Your name is Alice.\"}\n",
        "#    Turn 3:\n",
        "#      - input: {\"input\": \"What's the capital of France?\"}\n",
        "#      - output: {\"output\": \"The capital of France is Paris.\"}\n",
        "#\n",
        "# 3. Load and print the memory variables:\n",
        "#    - Use buffer_memory.load_memory_variables({})\n",
        "#    - Print the result with a label like \"ðŸ“ Buffer Memory:\"\n",
        "#\n",
        "# 4. Print the memory_variables attribute:\n",
        "#    - Print buffer_memory.memory_variables with a label like \"ðŸ’¾ Memory keys:\"\n",
        "\n",
        "raise NotImplementedError(\"Complete the ConversationBufferMemory exercise\")\n",
        "### END CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. ConversationSummaryMemory\n",
        "\n",
        "Instead of storing full history, **summarizes** past conversation using an LLM. Reduces token usage but may lose details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b5/fr4bwywx1hl34z7s66ljm40r0000gn/T/ipykernel_5324/1045286892.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  summary_memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ Summary Memory:\n",
            "{'history': [SystemMessage(content='The human introduces herself as Alice. The AI responds by greeting Alice and expressing pleasure in meeting her, confirming her name. The human then asks about the capital of France, and the AI replies that it is Paris.', additional_kwargs={}, response_metadata={})]}\n",
            "\n",
            "ðŸ“Š Summary is more compact than full buffer\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Create summary memory (requires LLM)\n",
        "summary_memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
        "\n",
        "# Simulate same conversation\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"Hi, my name is Alice.\"},\n",
        "    {\"output\": \"Hello Alice! Nice to meet you.\"}\n",
        ")\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    {\"output\": \"Your name is Alice.\"}\n",
        ")\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"What's the capital of France?\"},\n",
        "    {\"output\": \"The capital of France is Paris.\"}\n",
        ")\n",
        "\n",
        "# View summarized history\n",
        "print(\"ðŸ“ Summary Memory:\")\n",
        "print(summary_memory.load_memory_variables({}))\n",
        "print(f\"\\nðŸ“Š Summary is more compact than full buffer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trade-offs: Buffer vs Summary\n",
        "\n",
        "| Memory Type | Pros | Cons |\n",
        "|-------------|------|------|\n",
        "| **Buffer** | Full detail, no LLM calls | Grows unbounded, context limits |\n",
        "| **Summary** | Compact, scalable | LLM calls needed, possible drift |\n",
        "\n",
        "**When to use:**\n",
        "- **Buffer**: Short conversations, need exact history\n",
        "- **Summary**: Long conversations, want cost efficiency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section B: Memory in Chains\n",
        "\n",
        "Let's inject memory into a simple conversational chain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b5/fr4bwywx1hl34z7s66ljm40r0000gn/T/ipykernel_5324/2397125044.py:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ—¨ï¸  Conversational Chain with Memory\n",
            "\n",
            "User: Hi, I'm Bob and I love Python programming.\n",
            "AI: Hello, Bob! It's great to meet you! Python is such a versatile and powerful programming language. What do you enjoy most about it? Are you working on any specific projects or using any particular libraries? Thereâ€™s so much you can do with Python, from web development with frameworks like Django and Flask to data analysis with libraries like Pandas and NumPy.\n",
            "\n",
            "User: What's my name?\n",
            "AI: Your name is Bob! It's nice to chat with you. How did you get into Python programming?\n",
            "\n",
            "User: What do I love?\n",
            "AI: You love Python programming! It's fantastic to hear that. Python has so many applications and a vibrant community. What aspects of Python do you find most enjoyable? Is it the simplicity of the syntax, the vast array of libraries, or perhaps the community support?\n",
            "\n",
            "ðŸ“ Stored Memory:\n",
            "{'history': \"Human: Hi, I'm Bob and I love Python programming.\\nAI: Hello, Bob! It's great to meet you! Python is such a versatile and powerful programming language. What do you enjoy most about it? Are you working on any specific projects or using any particular libraries? Thereâ€™s so much you can do with Python, from web development with frameworks like Django and Flask to data analysis with libraries like Pandas and NumPy.\\nHuman: What's my name?\\nAI: Your name is Bob! It's nice to chat with you. How did you get into Python programming?\\nHuman: What do I love?\\nAI: You love Python programming! It's fantastic to hear that. Python has so many applications and a vibrant community. What aspects of Python do you find most enjoyable? Is it the simplicity of the syntax, the vast array of libraries, or perhaps the community support?\"}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Create a conversational chain with memory\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False,  # Set to True to see internal prompts\n",
        ")\n",
        "\n",
        "# Multi-turn conversation\n",
        "print(\"ðŸ—¨ï¸  Conversational Chain with Memory\\n\")\n",
        "\n",
        "response1 = conversation.predict(input=\"Hi, I'm Bob and I love Python programming.\")\n",
        "print(f\"User: Hi, I'm Bob and I love Python programming.\")\n",
        "print(f\"AI: {response1}\\n\")\n",
        "\n",
        "response2 = conversation.predict(input=\"What's my name?\")\n",
        "print(f\"User: What's my name?\")\n",
        "print(f\"AI: {response2}\\n\")\n",
        "\n",
        "response3 = conversation.predict(input=\"What do I love?\")\n",
        "print(f\"User: What do I love?\")\n",
        "print(f\"AI: {response3}\\n\")\n",
        "\n",
        "# View memory\n",
        "print(\"ðŸ“ Stored Memory:\")\n",
        "print(memory.load_memory_variables({}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resetting Memory\n",
        "\n",
        "Between sessions, clear memory to start fresh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After clearing memory:\n",
            "User: What's my name?\n",
            "AI: Iâ€™m not sure what your name is since I donâ€™t have access to that information. But Iâ€™d love to know it if youâ€™d like to share! What do you like to be called?\n",
            "\n",
            "âœ… Memory reset - AI no longer remembers Bob\n"
          ]
        }
      ],
      "source": [
        "# Clear memory\n",
        "memory.clear()\n",
        "\n",
        "response4 = conversation.predict(input=\"What's my name?\")\n",
        "print(f\"After clearing memory:\")\n",
        "print(f\"User: What's my name?\")\n",
        "print(f\"AI: {response4}\")\n",
        "print(f\"\\nâœ… Memory reset - AI no longer remembers Bob\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section C: LCEL (LangChain Expression Language) Basics\n",
        "\n",
        "LCEL is a declarative way to compose LangChain components using the `|` operator.\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "1. **Runnable**: Base interface for all LCEL components\n",
        "2. **Pipe (`|`)**: Chain runnables together\n",
        "3. **RunnablePassthrough**: Pass data through unchanged\n",
        "4. **RunnableMap**: Apply multiple operations in parallel\n",
        "\n",
        "### Simple LCEL Chain\n",
        "\n",
        "Let's build: `PromptTemplate | LLM | StrOutputParser`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”— Simple LCEL Chain:\n",
            "Question: What is eczema and how is it treated?\n",
            "Answer: Eczema, also known as atopic dermatitis, is a chronic inflammatory skin condition characterized by dry, itchy, and inflamed skin. It often occurs in flare-ups and can affect people of all ages.\n",
            "\n",
            "**Treatment options include:**\n",
            "\n",
            "1. **Moisturizers:** Regular use of emollients to keep the skin hydrated.\n",
            "2. **Topical corticosteroids:** To reduce inflammation and itching during flare-ups.\n",
            "3. **Topical calcineurin inhibitors:** Non-steroidal medications that help reduce inflammation.\n",
            "4. **Antihistamines:** To alleviate itching, especially at night.\n",
            "5. **Phototherapy:** Controlled exposure to ultraviolet light for severe cases.\n",
            "6. **Systemic medications:** In severe cases, oral or injectable medications may be prescribed.\n",
            "7. **Avoiding triggers:** Identifying and avoiding irritants or allergens that can worsen symptoms.\n",
            "\n",
            "Consulting a healthcare provider for a personalized treatment plan is recommended.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Define prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Answer concisely.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Build LCEL chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke\n",
        "response = chain.invoke({\"question\": \"What is eczema and how is it treated?\"})\n",
        "print(\"ðŸ”— Simple LCEL Chain:\")\n",
        "print(f\"Question: What is eczema and how is it treated?\")\n",
        "print(f\"Answer: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RunnablePassthrough & RunnableMap\n",
        "\n",
        "Use `RunnablePassthrough` to pass input data and `RunnableMap` (via dict) for parallel operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”— Chain with Context:\n",
            "Answer: Machine learning (ML) is a field of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "### START CODE HERE ### (â‰ˆ 20-25 lines)\n",
        "# YOUR CODE HERE\n",
        "# HINTS:\n",
        "# 1. Create a prompt template using ChatPromptTemplate.from_template():\n",
        "#    - The template should have placeholders: {context} and {question}\n",
        "#    - Example format:\n",
        "#      \"\"\"Use the context to answer the question.\n",
        "#      \n",
        "#      Context: {context}\n",
        "#      Question: {question}\n",
        "#      \n",
        "#      Answer:\"\"\"\n",
        "#    - Store in variable: context_prompt\n",
        "#\n",
        "# 2. Build an LCEL chain using the | (pipe) operator:\n",
        "#    chain_with_context = (\n",
        "#        RunnableParallel({\n",
        "#            \"context\": RunnablePassthrough(),   # Pass context through\n",
        "#            \"question\": RunnablePassthrough(),  # Pass question through\n",
        "#        })\n",
        "#        | context_prompt    # Format the prompt\n",
        "#        | llm               # Generate answer\n",
        "#        | StrOutputParser() # Extract string from response\n",
        "#    )\n",
        "#\n",
        "# 3. Test the chain by invoking with:\n",
        "#    result = chain_with_context.invoke({\n",
        "#        \"context\": \"Machine learning is a field of AI that learns from data.\",\n",
        "#        \"question\": \"What is ML?\"\n",
        "#    })\n",
        "#\n",
        "# 4. Print the result with label \"ðŸ”— Chain with Context:\"\n",
        "\n",
        "raise NotImplementedError(\"Complete the LCEL chain with context exercise\")\n",
        "### END CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section D: Advanced LCEL Patterns\n",
        "\n",
        "### 1. Streaming\n",
        "\n",
        "Stream tokens as they're generated for better UX.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŒŠ Streaming response:\n",
            "Answer: RAG (Retrieval-Augmented Generation) is a machine learning approach that combines retrieval of relevant documents from a knowledge base with generative models to produce more accurate and contextually relevant responses.\n",
            "\n",
            "âœ… Streaming complete\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(\"ðŸŒŠ Streaming response:\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "\n",
        "### START CODE HERE ### (â‰ˆ 5-7 lines)\n",
        "# YOUR CODE HERE\n",
        "# HINTS:\n",
        "# 1. Use chain.stream() with input: {\"question\": \"Explain RAG in one sentence.\"}\n",
        "# 2. Loop through the stream using: for chunk in chain.stream(...):\n",
        "# 3. Print each chunk without newline: print(chunk, end=\"\", flush=True)\n",
        "#    - end=\"\" prevents adding newline after each chunk\n",
        "#    - flush=True ensures immediate output\n",
        "# 4. After the loop, print two newlines and \"âœ… Streaming complete\"\n",
        "\n",
        "raise NotImplementedError(\"Complete the streaming exercise\")\n",
        "### END CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Retry with Fallback\n",
        "\n",
        "Use `.with_retry()` for automatic retries and `.with_fallbacks()` for fallback models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Chain with retry enabled\n",
            "If LLM fails, will retry up to 3 times\n",
            "âœ… Retry pattern configured\n"
          ]
        }
      ],
      "source": [
        "# Add retry logic (max 3 attempts)\n",
        "chain_with_retry = chain.with_retry(stop_after_attempt=3)\n",
        "\n",
        "print(\"ðŸ”„ Chain with retry enabled\")\n",
        "print(\"If LLM fails, will retry up to 3 times\")\n",
        "\n",
        "# Fallback example (requires a second LLM)\n",
        "# Uncomment if you have multiple providers configured\n",
        "# fallback_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "# chain_with_fallback = chain.with_fallbacks([fallback_chain])\n",
        "# print(\"âœ… Fallback chain: tries primary LLM, falls back to gpt-3.5-turbo\")\n",
        "\n",
        "print(\"âœ… Retry pattern configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section E: Minimal Tool Use (Optional)\n",
        "\n",
        "LCEL can invoke tools. Here's a simple example with a current time tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”§ Tool Demo:\n",
            "Current time: 2026-01-11T08:33:26.464479\n",
            "\n",
            "ðŸ”— Tool Chain Result:\n",
            "Answer: The time is 08:33 on January 11, 2026.\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "### START CODE HERE ### (â‰ˆ 25-30 lines)\n",
        "# YOUR CODE HERE\n",
        "# HINTS:\n",
        "# Part 1: Create a tool\n",
        "# 1. Use the @tool decorator\n",
        "# 2. Define function: def get_current_time() -> str:\n",
        "# 3. Add docstring: \"\"\"Returns the current time in ISO format.\"\"\"\n",
        "# 4. Import datetime: from datetime import datetime\n",
        "# 5. Return: datetime.now().isoformat()\n",
        "#\n",
        "# Part 2: Test the tool\n",
        "# 6. Print \"ðŸ”§ Tool Demo:\"\n",
        "# 7. Invoke the tool: get_current_time.invoke({})\n",
        "# 8. Print the result with label \"Current time:\"\n",
        "#\n",
        "# Part 3: Build a tool chain\n",
        "# 9. Create a ChatPromptTemplate with:\n",
        "#    - Placeholders: {question} and {tool_result}\n",
        "#    - Instructions mentioning the tool\n",
        "#    - Store in: tool_prompt\n",
        "#\n",
        "# 10. Build an LCEL chain:\n",
        "#     tool_chain = (\n",
        "#         RunnableParallel({\n",
        "#             \"question\": RunnablePassthrough(),\n",
        "#             \"tool_result\": RunnableLambda(lambda x: get_current_time.invoke({}))\n",
        "#         })\n",
        "#         | tool_prompt\n",
        "#         | llm\n",
        "#         | StrOutputParser()\n",
        "#     )\n",
        "#\n",
        "# 11. Test with: tool_chain.invoke({\"question\": \"What time is it?\"})\n",
        "# 12. Print the answer with label \"ðŸ”— Tool Chain Result:\"\n",
        "\n",
        "raise NotImplementedError(\"Complete the tool integration exercise\")\n",
        "### END CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Manifest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Manifest saved: artifacts/manifests/memory_lcel.json\n"
          ]
        }
      ],
      "source": [
        "manifests_dir = Path(config[\"artifacts_root\"]) / \"manifests\"\n",
        "manifests_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "manifest = {\n",
        "    \"notebook\": \"12_memory_lcel_basics\",\n",
        "    \"topics\": [\n",
        "        \"ConversationBufferMemory\",\n",
        "        \"ConversationSummaryMemory\",\n",
        "        \"ConversationChain with memory\",\n",
        "        \"LCEL composition\",\n",
        "        \"Streaming\",\n",
        "        \"Retry patterns\",\n",
        "        \"Tool use\"\n",
        "    ],\n",
        "    \"llm_provider\": config[\"llm_provider\"],\n",
        "    \"llm_model\": config[\"llm_model\"],\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "manifest_path = manifests_dir / \"memory_lcel.json\"\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(f\"âœ… Manifest saved: {manifest_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**What we learned:**\n",
        "\n",
        "### Memory\n",
        "- âœ… **Buffer Memory**: Stores full history (simple but grows)\n",
        "- âœ… **Summary Memory**: LLM-summarized history (compact but may drift)\n",
        "- âœ… **Memory in Chains**: Inject context into conversational flows\n",
        "- âœ… **Reset/Clear**: Start fresh between sessions\n",
        "\n",
        "### LCEL\n",
        "- âœ… **Composition**: Use `|` to chain runnables\n",
        "- âœ… **RunnablePassthrough**: Pass data unchanged\n",
        "- âœ… **RunnableParallel**: Run operations in parallel\n",
        "- âœ… **Streaming**: Token-by-token generation\n",
        "- âœ… **Retry**: Automatic retries on failure\n",
        "- âœ… **Fallbacks**: Switch to backup LLM\n",
        "- âœ… **Tools**: Integrate external functions\n",
        "\n",
        "**Key Patterns:**\n",
        "```python\n",
        "# Simple chain\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# With context\n",
        "chain = RunnableParallel({...}) | prompt | llm | parser\n",
        "\n",
        "# With retry\n",
        "chain = chain.with_retry(stop_after_attempt=3)\n",
        "\n",
        "# With streaming\n",
        "for chunk in chain.stream(input):\n",
        "    print(chunk)\n",
        "```\n",
        "\n",
        "**Artifacts:**\n",
        "- `./artifacts/manifests/memory_lcel.json`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
