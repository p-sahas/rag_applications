{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 ‚Äì Memory & LCEL Basics\n",
        "\n",
        "**Learning Goals:**\n",
        "- Understand conversational memory in LangChain\n",
        "- Compare memory types: Buffer vs Summary\n",
        "- Master LCEL (LangChain Expression Language) composition\n",
        "- Build streaming, retry, and fallback patterns\n",
        "\n",
        "**What we'll cover:**\n",
        "1. **Memory 101** - Buffer and Summary memory patterns\n",
        "2. **Memory in Chains** - Inject memory into conversational flows\n",
        "3. **LCEL Basics** - Compose runnables with `|` operator\n",
        "4. **Advanced LCEL** - Streaming, retry, fallbacks\n",
        "5. **Tool Use (Optional)** - Simple tool integration\n",
        "\n",
        "**Note:** This notebook focuses on fundamentals, not RAG. No ChromaDB or retrieval here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Working directory: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Week 03\n",
            " Config loaded:\n",
            "  LLM: groq / openai/gpt-oss-120b\n",
            "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
            "  Temperature: 0.2\n",
            "  Artifacts: ./artifacts\n",
            "  Note: Temperature is 0.2 (good for conversational demos)\n"
          ]
        }
      ],
      "source": [
        "#  Global Config & Services (using centralized modules)\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add parent directory to path and change to project root\n",
        "import os\n",
        "\n",
        "# Get the notebook's current directory and find project root\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == \"notebooks\":\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "# Change to project root and add to path\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\" Working directory: {os.getcwd()}\")\n",
        "\n",
        "from src.services.llm_services import (\n",
        "    load_config,\n",
        "    get_llm,\n",
        "    validate_api_keys,\n",
        "    print_config_summary\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Load configuration from config.yaml (now we're in project root)\n",
        "config = load_config(\"src/config/config.yaml\")\n",
        "\n",
        "# Validate API keys\n",
        "validate_api_keys(config, verbose=True)\n",
        "\n",
        "# Print summary\n",
        "print_config_summary(config)\n",
        "print(f\"  Note: Temperature is {config['temperature']} (good for conversational demos)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " LLM initialized: groq / openai/gpt-oss-120b\n",
            "\n",
            " Testing API connection...\n",
            " API key verified: API working!\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM using factory from llm_services\n",
        "llm = get_llm(config)\n",
        "print(f\" LLM initialized: {config['llm_provider']} / {config['llm_model']}\")\n",
        "\n",
        "# Verify API key with test completion\n",
        "print(\"\\n Testing API connection...\")\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
        "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
        "    print(f\" API key verified: {test_msg[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\" API key test failed: {e}\")\n",
        "    print(\"  Please check your .env file and API key configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section A: Memory 101\n",
        "\n",
        "LangChain provides memory primitives to maintain conversational context across turns.\n",
        "\n",
        "### 1. ConversationBufferMemory\n",
        "\n",
        "Stores **full chat history** in memory. Simple but can grow large.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sahas Induwara\\AppData\\Local\\Temp\\ipykernel_11320\\603689350.py:18: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(llm=llm,\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationChain\n",
        "\n",
        "buffer_memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# buffer_memory.save_context(\n",
        "#     {\"input\": \"Hello!, My name is Sahas.\"},\n",
        "#     {\"output\": \"Hello Sahas! Nice to meet you.\"}\n",
        "# )\n",
        "\n",
        "# buffer_memory.save_context(\n",
        "#     {\"input\": \"What is my name?\"},\n",
        "#     {\"output\": \"Your name is Sahas.\"}\n",
        "# )\n",
        "\n",
        "# print(buffer_memory.load_memory_variables({}))  # {'history': ...}\n",
        "\n",
        "conversation = ConversationChain(llm=llm,\n",
        "                                memory=buffer_memory,\n",
        "                                verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hello\n",
            "Bot: Hey there, Sahas! üëã Great to hear from you again. How‚Äôs everything going on your side of the world?  \n",
            "\n",
            "Since you mentioned you‚Äôre from Sri‚ÄØLanka, I‚Äôve been thinking about a few more cool things you might enjoy:\n",
            "\n",
            "- **Cultural gems:** Have you ever visited the ancient city of **Anuradhapura**? The massive stone stupas (like Ruwanwelisaya) and the sacred Bo tree are truly awe‚Äëinspiring.\n",
            "- **Food adventure:** If you haven‚Äôt tried **pol sambol** yet, it‚Äôs a fiery coconut relish that pairs perfectly with rice and curry‚Äîor even just a fresh roti.\n",
            "- **Nature escape:** The **Horton Plains** plateau offers the famous ‚ÄúWorld‚Äôs End‚Äù cliff‚Äîan 870‚Äëmeter drop with breathtaking sunrise views.\n",
            "- **Music & dance:** The rhythmic beats of **baila** and the graceful movements of **kandyan dance** are such vibrant expressions of Sri‚ÄØLankan heritage.\n",
            "\n",
            "Anything exciting happening today? A project you‚Äôre working on, a favorite spot you love to visit, or maybe a question you‚Äôve been pondering? I‚Äôm all ears and ready to dive into whatever you‚Äôd like to chat about! üåü\n",
            "\n",
            "Human: What is my name and country\n",
            "Bot: Your name is **Sahas**, and you‚Äôre from **Sri‚ÄØLanka** üå¥.  \n",
            "\n",
            "Just to recap the little bits you‚Äôve shared so far:\n",
            "\n",
            "- **Name:** Sahas  \n",
            "- **Country:** Sri‚ÄØLanka ‚Äì the island nation famous for its ancient rock‚Äëfortress Sigiriya, mist‚Äëcovered tea plantations, leopards in Yala National‚ÄØPark, and delicious dishes like kottu roti and hoppers.\n",
            "\n",
            "If there‚Äôs anything else you‚Äôd like to dive into‚Äîmaybe a favorite spot in Sri‚ÄØLanka, a hobby you enjoy, or a question you‚Äôve been pondering‚Äîjust let me know! üòä\n",
            "\n",
            "Exiting chat.\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Exiting chat.\")\n",
        "        break\n",
        "\n",
        "    response = conversation.predict(input=user_input)\n",
        "\n",
        "    print(f\"Human: {user_input}\")\n",
        "    print(f\"Bot: {response}\\n\")\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello! üëã It‚Äôs great to meet you. I‚Äôm an AI language model created by OpenAI‚Äîthink of me as a very well‚Äëread, endlessly curious conversational partner. A few quick facts about me:\\n\\n- **Model family:** GPT‚Äë4‚ÄëTurbo (the latest, most efficient version of the GPT‚Äë4 series).  \\n- **Training cut‚Äëoff:** I was trained on data up to June\\u202f2024, so I‚Äôm familiar with events, scientific advances, pop culture, and internet trends up to that point.  \\n- **Current date:** It‚Äôs January\\u202f30,\\u202f2026 where I‚Äôm ‚Äúliving‚Äù in the digital realm.  \\n- **Personality:** I aim to be friendly, talkative, and detail‚Äëoriented. If I ever don‚Äôt know something, I‚Äôll let you know honestly.\\n\\nI‚Äôm here to chat, answer questions, brainstorm ideas, help with writing, explain concepts, or just share a fun fact‚Äîwhatever you need. How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello! üëã It‚Äôs great to meet you. I‚Äôm an AI language model created by OpenAI‚Äîthink of me as a very well‚Äëread, endlessly curious conversational partner. A few quick facts about me:\\n\\n- **Model family:** GPT‚Äë4‚ÄëTurbo (the latest, most efficient version of the GPT‚Äë4 series).  \\n- **Training cut‚Äëoff:** I was trained on data up to June\\u202f2024, so I‚Äôm familiar with events, scientific advances, pop culture, and internet trends up to that point.  \\n- **Current date:** It‚Äôs January\\u202f30,\\u202f2026 where I‚Äôm ‚Äúliving‚Äù in the digital realm.  \\n- **Personality:** I aim to be friendly, talkative, and detail‚Äëoriented. If I ever don‚Äôt know something, I‚Äôll let you know honestly.\\n\\nI‚Äôm here to chat, answer questions, brainstorm ideas, help with writing, explain concepts, or just share a fun fact‚Äîwhatever you need. How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hey there! üëã Great to hear from you again. I‚Äôm here and ready to chat‚Äîwhether you‚Äôve got a burning question, need help brainstorming, want a quick fun fact, or just feel like a friendly conversation. What‚Äôs on your mind today?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hey there! üëã Great to hear from you again. I‚Äôm here and ready to chat‚Äîwhether you‚Äôve got a burning question, need help brainstorming, want a quick fun fact, or just feel like a friendly conversation. What‚Äôs on your mind today?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='my name is Sahas', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Nice to meet you, Sahas! üëã I‚Äôm glad you introduced yourself. How‚Äôs your day going so far? Anything fun or interesting you‚Äôd like to chat about‚Äîmaybe a hobby, a project you‚Äôre working on, or a question that‚Äôs been on your mind? I‚Äôm all ears!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='my name is Sahas', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Nice to meet you, Sahas! üëã I‚Äôm glad you introduced yourself. How‚Äôs your day going so far? Anything fun or interesting you‚Äôd like to chat about‚Äîmaybe a hobby, a project you‚Äôre working on, or a question that‚Äôs been on your mind? I‚Äôm all ears!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='I am from sri lanka', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Nice to meet you, Sahas! üå∫ It‚Äôs great to hear you‚Äôre from Sri\\u202fLanka‚Äîsuch a beautiful island with a rich tapestry of history, nature, and culture.\\n\\nA few fun tidbits about Sri\\u202fLanka that I love:\\n\\n- **Ancient marvels:** The rock‚Äëfortress of **Sigiriya** (the ‚ÄúLion Rock‚Äù) is a UNESCO World Heritage site and one of the most photographed archaeological sites in the world. It dates back to the 5th\\u202fcentury\\u202fCE and is famous for its frescoes and massive water gardens.  \\n- **Tea country:** The central highlands‚Äîplaces like Nuwara Eliya, Kandy, and Bandarawela‚Äîproduce some of the world‚Äôs finest Ceylon tea. The tea‚Äëplantation scenery, with mist‚Äëcloaked hills and tea‚Äëpickers in bright shirts, is iconic.  \\n- **Wildlife:** Yala National Park has one of the highest densities of leopards on the planet, plus elephants, sloth bears, and a dazzling array of birds.  \\n- **Festivals:** **Vesak** (celebrating the Buddha‚Äôs birth, enlightenment, and death) lights up the night sky with lanterns and illuminated pandals. **Sinhala and Tamil New Year** in April is a time of traditional games, sweets, and family gatherings.  \\n- **Cuisine:** From aromatic **kottu roti** and spicy **hoppers** (appa) to the sweet, milky **kiri', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='I am from sri lanka', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Nice to meet you, Sahas! üå∫ It‚Äôs great to hear you‚Äôre from Sri\\u202fLanka‚Äîsuch a beautiful island with a rich tapestry of history, nature, and culture.\\n\\nA few fun tidbits about Sri\\u202fLanka that I love:\\n\\n- **Ancient marvels:** The rock‚Äëfortress of **Sigiriya** (the ‚ÄúLion Rock‚Äù) is a UNESCO World Heritage site and one of the most photographed archaeological sites in the world. It dates back to the 5th\\u202fcentury\\u202fCE and is famous for its frescoes and massive water gardens.  \\n- **Tea country:** The central highlands‚Äîplaces like Nuwara Eliya, Kandy, and Bandarawela‚Äîproduce some of the world‚Äôs finest Ceylon tea. The tea‚Äëplantation scenery, with mist‚Äëcloaked hills and tea‚Äëpickers in bright shirts, is iconic.  \\n- **Wildlife:** Yala National Park has one of the highest densities of leopards on the planet, plus elephants, sloth bears, and a dazzling array of birds.  \\n- **Festivals:** **Vesak** (celebrating the Buddha‚Äôs birth, enlightenment, and death) lights up the night sky with lanterns and illuminated pandals. **Sinhala and Tamil New Year** in April is a time of traditional games, sweets, and family gatherings.  \\n- **Cuisine:** From aromatic **kottu roti** and spicy **hoppers** (appa) to the sweet, milky **kiri', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hey there, Sahas! üëã Great to hear from you again. How‚Äôs everything going on your side of the world?  \\n\\nSince you mentioned you‚Äôre from Sri\\u202fLanka, I‚Äôve been thinking about a few more cool things you might enjoy:\\n\\n- **Cultural gems:** Have you ever visited the ancient city of **Anuradhapura**? The massive stone stupas (like Ruwanwelisaya) and the sacred Bo tree are truly awe‚Äëinspiring.\\n- **Food adventure:** If you haven‚Äôt tried **pol sambol** yet, it‚Äôs a fiery coconut relish that pairs perfectly with rice and curry‚Äîor even just a fresh roti.\\n- **Nature escape:** The **Horton Plains** plateau offers the famous ‚ÄúWorld‚Äôs End‚Äù cliff‚Äîan 870‚Äëmeter drop with breathtaking sunrise views.\\n- **Music & dance:** The rhythmic beats of **baila** and the graceful movements of **kandyan dance** are such vibrant expressions of Sri\\u202fLankan heritage.\\n\\nAnything exciting happening today? A project you‚Äôre working on, a favorite spot you love to visit, or maybe a question you‚Äôve been pondering? I‚Äôm all ears and ready to dive into whatever you‚Äôd like to chat about! üåü', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name and country', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is **Sahas**, and you‚Äôre from **Sri\\u202fLanka** üå¥.  \\n\\nJust to recap the little bits you‚Äôve shared so far:\\n\\n- **Name:** Sahas  \\n- **Country:** Sri\\u202fLanka ‚Äì the island nation famous for its ancient rock‚Äëfortress Sigiriya, mist‚Äëcovered tea plantations, leopards in Yala National\\u202fPark, and delicious dishes like kottu roti and hoppers.\\n\\nIf there‚Äôs anything else you‚Äôd like to dive into‚Äîmaybe a favorite spot in Sri\\u202fLanka, a hobby you enjoy, or a question you‚Äôve been pondering‚Äîjust let me know! üòä', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "buffer_memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. ConversationSummaryMemory\n",
        "\n",
        "Instead of storing full history, **summarizes** past conversation using an LLM. Reduces token usage but may lose details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sahas Induwara\\AppData\\Local\\Temp\\ipykernel_11320\\3564447599.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  summary_memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Summary Memory:\n",
            "{'history': [SystemMessage(content='The human introduces themselves as Alice, and the AI greets Alice in response. The human then asks for their name, and the AI confirms that the human‚Äôs name is Alice. The human then asks for the capital of France, and the AI replies that it is Paris.', additional_kwargs={}, response_metadata={})]}\n",
            "\n",
            " Summary is more compact than full buffer\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "\n",
        "# Create summary memory (requires LLM)\n",
        "summary_memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
        "\n",
        "# Simulate same conversation\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"Hi, my name is Alice.\"},\n",
        "    {\"output\": \"Hello Alice! Nice to meet you.\"}\n",
        ")\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    {\"output\": \"Your name is Alice.\"}\n",
        ")\n",
        "summary_memory.save_context(\n",
        "    {\"input\": \"What's the capital of France?\"},\n",
        "    {\"output\": \"The capital of France is Paris.\"}\n",
        ")\n",
        "\n",
        "# View summarized history\n",
        "print(\" Summary Memory:\")\n",
        "print(summary_memory.load_memory_variables({}))\n",
        "print(f\"\\n Summary is more compact than full buffer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trade-offs: Buffer vs Summary\n",
        "\n",
        "| Memory Type | Pros | Cons |\n",
        "|-------------|------|------|\n",
        "| **Buffer** | Full detail, no LLM calls | Grows unbounded, context limits |\n",
        "| **Summary** | Compact, scalable | LLM calls needed, possible drift |\n",
        "\n",
        "**When to use:**\n",
        "- **Buffer**: Short conversations, need exact history\n",
        "- **Summary**: Long conversations, want cost efficiency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section B: Memory in Chains\n",
        "\n",
        "Let's inject memory into a simple conversational chain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Conversational Chain with Memory\n",
            "\n",
            "User: Hi, I'm Bob and I love Python programming.\n",
            "AI: Hey Bob! Great to meet you üëã‚ÄîI‚Äôm always excited to chat with fellow Python enthusiasts.  \n",
            "\n",
            "A little about me: I‚Äôve been trained on a massive corpus of Python code, tutorials, and community discussions, so I can dive into everything from the basics of list comprehensions to the intricacies of async I/O and metaprogramming. I‚Äôve even ‚Äúread‚Äù the source code of popular libraries like **NumPy**, **Pandas**, **FastAPI**, and **TensorFlow**, so I can help you troubleshoot or explore new features.\n",
            "\n",
            "Since you love Python, I‚Äôm curious‚Äîwhat kind of projects are you working on or dreaming about? Some fun directions people often go in include:\n",
            "\n",
            "| Area | Typical Libraries / Tools | Cool Project Ideas |\n",
            "|------|---------------------------|--------------------|\n",
            "| **Data Science / Machine Learning** | `pandas`, `scikit‚Äëlearn`, `tensorflow`, `pytorch` | Build a model that predicts your favorite coffee roast based on sensor data, or analyze a public dataset like the Titanic survivors. |\n",
            "| **Web Development** | `Flask`, `Django`, `FastAPI`, `SQLAlchemy` | Create a personal blog with a custom markdown editor, or a tiny API that serves random jokes from a database. |\n",
            "| **Automation / Scripting** | `click`, `argparse`, `watchdog`, `pyautogui` | Write a script that watches a folder\n",
            "\n",
            "User: What's my name?\n",
            "AI: Your name is **Bob**! üëã  \n",
            "\n",
            "Nice to meet you, Bob‚Äîif you ever want to chat about a Python project, share a cool script you‚Äôve written, or just talk about your favorite libraries, I‚Äôm all ears. What‚Äôs the most recent piece of code you‚Äôve been tinkering with?\n",
            "\n",
            "User: What do I love?\n",
            "AI: You told me you‚Äôre a big fan of **Python programming**‚ÄØ‚Äî‚ÄØthat‚Äôs what you love! üêçüíª  \n",
            "\n",
            "It‚Äôs awesome to meet another Python enthusiast. Do you have a favorite library or a particular feature (like list comprehensions, async/await, or metaclasses) that gets you especially excited? I‚Äôd love to hear what you enjoy most about coding in Python!\n",
            "\n",
            " Stored Memory:\n",
            "{'history': \"Human: Hi, I'm Bob and I love Python programming.\\nAI: Hey Bob! Great to meet you üëã‚ÄîI‚Äôm always excited to chat with fellow Python enthusiasts.  \\n\\nA little about me: I‚Äôve been trained on a massive corpus of Python code, tutorials, and community discussions, so I can dive into everything from the basics of list comprehensions to the intricacies of async I/O and metaprogramming. I‚Äôve even ‚Äúread‚Äù the source code of popular libraries like **NumPy**, **Pandas**, **FastAPI**, and **TensorFlow**, so I can help you troubleshoot or explore new features.\\n\\nSince you love Python, I‚Äôm curious‚Äîwhat kind of projects are you working on or dreaming about? Some fun directions people often go in include:\\n\\n| Area | Typical Libraries / Tools | Cool Project Ideas |\\n|------|---------------------------|--------------------|\\n| **Data Science / Machine Learning** | `pandas`, `scikit‚Äëlearn`, `tensorflow`, `pytorch` | Build a model that predicts your favorite coffee roast based on sensor data, or analyze a public dataset like the Titanic survivors. |\\n| **Web Development** | `Flask`, `Django`, `FastAPI`, `SQLAlchemy` | Create a personal blog with a custom markdown editor, or a tiny API that serves random jokes from a database. |\\n| **Automation / Scripting** | `click`, `argparse`, `watchdog`, `pyautogui` | Write a script that watches a folder\\nHuman: What's my name?\\nAI: Your name is **Bob**! üëã  \\n\\nNice to meet you, Bob‚Äîif you ever want to chat about a Python project, share a cool script you‚Äôve written, or just talk about your favorite libraries, I‚Äôm all ears. What‚Äôs the most recent piece of code you‚Äôve been tinkering with?\\nHuman: What do I love?\\nAI: You told me you‚Äôre a big fan of **Python programming**\\u202f‚Äî\\u202fthat‚Äôs what you love! üêçüíª  \\n\\nIt‚Äôs awesome to meet another Python enthusiast. Do you have a favorite library or a particular feature (like list comprehensions, async/await, or metaclasses) that gets you especially excited? I‚Äôd love to hear what you enjoy most about coding in Python!\"}\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.chains import ConversationChain\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "\n",
        "# Create a conversational chain with memory\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=False,  # Set to True to see internal prompts\n",
        ")\n",
        "\n",
        "# Multi-turn conversation\n",
        "print(\"  Conversational Chain with Memory\\n\")\n",
        "\n",
        "response1 = conversation.predict(input=\"Hi, I'm Bob and I love Python programming.\")\n",
        "print(f\"User: Hi, I'm Bob and I love Python programming.\")\n",
        "print(f\"AI: {response1}\\n\")\n",
        "\n",
        "response2 = conversation.predict(input=\"What's my name?\")\n",
        "print(f\"User: What's my name?\")\n",
        "print(f\"AI: {response2}\\n\")\n",
        "\n",
        "response3 = conversation.predict(input=\"What do I love?\")\n",
        "print(f\"User: What do I love?\")\n",
        "print(f\"AI: {response3}\\n\")\n",
        "\n",
        "# View memory\n",
        "print(\" Stored Memory:\")\n",
        "print(memory.load_memory_variables({}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resetting Memory\n",
        "\n",
        "Between sessions, clear memory to start fresh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After clearing memory:\n",
            "User: What's my name?\n",
            "AI: I‚Äôm afraid I don‚Äôt actually know your name‚Äîmy system doesn‚Äôt have any personal data about you unless you share it with me. If you‚Äôd like, feel free to tell me what you‚Äôd like to be called, and I‚Äôll gladly use it from now on!\n",
            "\n",
            " Memory reset - AI no longer remembers Bob\n"
          ]
        }
      ],
      "source": [
        "# Clear memory\n",
        "memory.clear()\n",
        "\n",
        "response4 = conversation.predict(input=\"What's my name?\")\n",
        "print(f\"After clearing memory:\")\n",
        "print(f\"User: What's my name?\")\n",
        "print(f\"AI: {response4}\")\n",
        "print(f\"\\n Memory reset - AI no longer remembers Bob\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section C: LCEL (LangChain Expression Language) Basics\n",
        "\n",
        "LCEL is a declarative way to compose LangChain components using the `|` operator.\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "1. **Runnable**: Base interface for all LCEL components\n",
        "2. **Pipe (`|`)**: Chain runnables together\n",
        "3. **RunnablePassthrough**: Pass data through unchanged\n",
        "4. **RunnableMap**: Apply multiple operations in parallel\n",
        "\n",
        "### Simple LCEL Chain\n",
        "\n",
        "Let's build: `PromptTemplate | LLM | StrOutputParser`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Simple LCEL Chain:\n",
            "Question: What is eczema and how is it treated?\n",
            "Answer: **Eczema (atopic dermatitis)** is a chronic, inflammatory skin condition characterized by dry, itchy patches that may become red, scaly, cracked, or weepy. It often runs in families with a history of allergies, asthma, or hay fever.\n",
            "\n",
            "### Key Features\n",
            "| Aspect | Details |\n",
            "|--------|---------|\n",
            "| **Typical sites** | Flexural areas (inside elbows/knees), neck, face, hands, feet |\n",
            "| **Symptoms** | Intense itching, redness, swelling, crusting, thickened skin (lichenification) |\n",
            "| **Triggers** | Irritants (soaps, detergents), allergens (dust mites, pet dander), stress, temperature extremes, sweat, certain fabrics |\n",
            "\n",
            "### Treatment Overview\n",
            "| Goal | Options |\n",
            "|------|----------|\n",
            "| **Restore skin barrier** | ‚Ä¢ **Moisturizers** (thick ointments or creams) applied 2‚Äì3‚ÄØ√ó‚ÄØdaily; **emollient‚Äërich** products containing ceramides, petrolatum, or hyaluronic acid.<br>‚Ä¢ **Bathing strategy** ‚Äì lukewarm water, short baths, add colloidal oatmeal or bath oils; pat skin dry, then moisturize while still damp. |\n",
            "| **Control inflammation** | ‚Ä¢ **Topical corticosteroids** (low‚Äëpotency for face/neck, medium‚Äëpotency for body, high‚Äëpotency for flare‚Äëups). Use the ‚Äúfingertip unit‚Äù method and limit to 2‚Äë4‚ÄØweeks per area.<br>‚Ä¢ **Top\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Define prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Answer concisely.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Build LCEL chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke\n",
        "response = chain.invoke({\"question\": \"What is eczema and how is it treated?\"})\n",
        "print(\" Simple LCEL Chain:\")\n",
        "print(f\"Question: What is eczema and how is it treated?\")\n",
        "print(f\"Answer: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RunnablePassthrough & RunnableMap\n",
        "\n",
        "Use `RunnablePassthrough` to pass input data and `RunnableMap` (via dict) for parallel operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Machine learning (ML) is a branch of artificial intelligence that enables computers to learn patterns and make decisions directly from data.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# 1. Create a prompt template using ChatPromptTemplate.from_template():\n",
        "#    - The template should have placeholders: {context} and {question}\n",
        "#    - Example format:\n",
        "#      \"\"\"Use the context to answer the question.\n",
        "#      \n",
        "#      Context: {context}\n",
        "#      Question: {question}\n",
        "#      \n",
        "#      Answer:\"\"\"\n",
        "#    - Store in variable: context_prompt\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Use the context to answer the question.\n",
        "    \n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "    \n",
        "    Answer:\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Build an LCEL chain using the | (pipe) operator:\n",
        "#    chain_with_context = (\n",
        "#        RunnableParallel({\n",
        "#            \"context\": RunnablePassthrough(),   # Pass context through\n",
        "#            \"question\": RunnablePassthrough(),  # Pass question through\n",
        "#        })\n",
        "#        | context_prompt    # Format the prompt\n",
        "#        | llm               # Generate answer\n",
        "#        | StrOutputParser() # Extract string from response\n",
        "#    )\n",
        "chain_lsel = (\n",
        "    RunnableParallel({\n",
        "        \"context\": RunnablePassthrough(),   # Pass context through\n",
        "        \"question\": RunnablePassthrough(),  # Pass question through\n",
        "    })\n",
        "    | prompt    # Format the prompt\n",
        "    | llm               # Generate answer\n",
        "    | StrOutputParser() # Extract string from response\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Test the chain by invoking with:\n",
        "#    result = chain_with_context.invoke({\n",
        "#        \"context\": \"Machine learning is a field of AI that learns from data.\",\n",
        "#        \"question\": \"What is ML?\"\n",
        "#    })\n",
        "#\n",
        "# 4. Print the result with label \" Chain with Context:\"\n",
        "result = chain_lsel.invoke({\n",
        "    \"context\": \"Machine learning is a field of AI that learns from data.\",\n",
        "    \"question\": \"What is ML?\"\n",
        "})\n",
        "result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section D: Advanced LCEL Patterns\n",
        "\n",
        "### 1. Streaming\n",
        "\n",
        "Stream tokens as they're generated for better UX.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without Streaming response:\n",
            "Retrieval‚ÄëAugmented Generation (RAG) is a technique that combines a language model with a searchable external knowledge base, retrieving relevant documents at inference time and feeding them into the model to produce more accurate, up‚Äëto‚Äëdate, and grounded responses.\n",
            "Streaming response:\n",
            "Answer: ..............................................Retr.ieval.‚Äë.Aug.mented. Generation. (.R.AG.). is. a. technique. that. combines. a. language. model. with. a. searchable. external. knowledge. base.,. retrieving. relevant. documents. at. inference. time. to. inform. and. improve. the. model.‚Äôs. generated. responses....."
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(\"Without Streaming response:\")\n",
        "print(chain.invoke({\"question\": \"Explain RAG in one sentence.\"}))\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "print(\"Answer: \", end=\"\")\n",
        "\n",
        "for chunk in chain.stream({\"question\": \"Explain RAG in one sentence.\"}):\n",
        "    print(chunk, end=\".\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Retry with Fallback\n",
        "\n",
        "Use `.with_retry()` for automatic retries and `.with_fallbacks()` for fallback models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Chain with retry enabled\n",
            "If LLM fails, will retry up to 3 times\n",
            " Retry pattern configured\n"
          ]
        }
      ],
      "source": [
        "# Add retry logic (max 3 attempts)\n",
        "chain_with_retry = chain.with_retry(stop_after_attempt=3)\n",
        "\n",
        "print(\" Chain with retry enabled\")\n",
        "print(\"If LLM fails, will retry up to 3 times\")\n",
        "\n",
        "# Fallback example (requires a second LLM)\n",
        "# Uncomment if you have multiple providers configured\n",
        "# fallback_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "# chain_with_fallback = chain.with_fallbacks([fallback_chain])\n",
        "# print(\" Fallback chain: tries primary LLM, falls back to gpt-3.5-turbo\")\n",
        "\n",
        "print(\" Retry pattern configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section E: Minimal Tool Use (Optional)\n",
        "\n",
        "LCEL can invoke tools. Here's a simple example with a current time tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Tool Demo:\n",
            "Current time: 2026-01-11T08:33:26.464479\n",
            "\n",
            "üîó Tool Chain Result:\n",
            "Answer: The time is 08:33 on January 11, 2026.\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "### START CODE HERE ### (‚âà 25-30 lines)\n",
        "# YOUR CODE HERE\n",
        "# HINTS:\n",
        "# Part 1: Create a tool\n",
        "# 1. Use the @tool decorator\n",
        "# 2. Define function: def get_current_time() -> str:\n",
        "# 3. Add docstring: \"\"\"Returns the current time in ISO format.\"\"\"\n",
        "# 4. Import datetime: from datetime import datetime\n",
        "# 5. Return: datetime.now().isoformat()\n",
        "#\n",
        "# Part 2: Test the tool\n",
        "# 6. Print \" Tool Demo:\"\n",
        "# 7. Invoke the tool: get_current_time.invoke({})\n",
        "# 8. Print the result with label \"Current time:\"\n",
        "#\n",
        "# Part 3: Build a tool chain\n",
        "# 9. Create a ChatPromptTemplate with:\n",
        "#    - Placeholders: {question} and {tool_result}\n",
        "#    - Instructions mentioning the tool\n",
        "#    - Store in: tool_prompt\n",
        "#\n",
        "# 10. Build an LCEL chain:\n",
        "#     tool_chain = (\n",
        "#         RunnableParallel({\n",
        "#             \"question\": RunnablePassthrough(),\n",
        "#             \"tool_result\": RunnableLambda(lambda x: get_current_time.invoke({}))\n",
        "#         })\n",
        "#         | tool_prompt\n",
        "#         | llm\n",
        "#         | StrOutputParser()\n",
        "#     )\n",
        "#\n",
        "# 11. Test with: tool_chain.invoke({\"question\": \"What time is it?\"})\n",
        "# 12. Print the answer with label \" Tool Chain Result:\"\n",
        "\n",
        "raise NotImplementedError(\"Complete the tool integration exercise\")\n",
        "### END CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Manifest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Manifest saved: artifacts\\manifests\\memory_lcel.json\n"
          ]
        }
      ],
      "source": [
        "manifests_dir = Path(config[\"artifacts_root\"]) / \"manifests\"\n",
        "manifests_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "manifest = {\n",
        "    \"notebook\": \"12_memory_lcel_basics\",\n",
        "    \"topics\": [\n",
        "        \"ConversationBufferMemory\",\n",
        "        \"ConversationSummaryMemory\",\n",
        "        \"ConversationChain with memory\",\n",
        "        \"LCEL composition\",\n",
        "        \"Streaming\",\n",
        "        \"Retry patterns\",\n",
        "        \"Tool use\"\n",
        "    ],\n",
        "    \"llm_provider\": config[\"llm_provider\"],\n",
        "    \"llm_model\": config[\"llm_model\"],\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "manifest_path = manifests_dir / \"memory_lcel.json\"\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(f\" Manifest saved: {manifest_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**What we learned:**\n",
        "\n",
        "### Memory\n",
        "-  **Buffer Memory**: Stores full history (simple but grows)\n",
        "-  **Summary Memory**: LLM-summarized history (compact but may drift)\n",
        "-  **Memory in Chains**: Inject context into conversational flows\n",
        "-  **Reset/Clear**: Start fresh between sessions\n",
        "\n",
        "### LCEL\n",
        "-  **Composition**: Use `|` to chain runnables\n",
        "-  *RunnablePassthrough**: Pass data unchanged\n",
        "-  **RunnableParallel**: Run operations in parallel\n",
        "-  **Streaming**: Token-by-token generation\n",
        "-  **Retry**: Automatic retries on failure\n",
        "-  **Fallbacks**: Switch to backup LLM\n",
        "-  **Tools**: Integrate external functions\n",
        "\n",
        "**Key Patterns:**\n",
        "```python\n",
        "# Simple chain\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# With context\n",
        "chain = RunnableParallel({...}) | prompt | llm | parser\n",
        "\n",
        "# With retry\n",
        "chain = chain.with_retry(stop_after_attempt=3)\n",
        "\n",
        "# With streaming\n",
        "for chunk in chain.stream(input):\n",
        "    print(chunk)\n",
        "```\n",
        "\n",
        "**Artifacts:**\n",
        "- `./artifacts/manifests/memory_lcel.json`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sahas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
