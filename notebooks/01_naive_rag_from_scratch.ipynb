{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 ‚Äì Naive RAG from Scratch\n",
        "\n",
        "**Learning Goals:**\n",
        "- Build a minimal RAG pipeline without frameworks\n",
        "- Understand TF-IDF vectorization and cosine similarity\n",
        "- See the core retrieve-then-generate pattern\n",
        "\n",
        "**What we'll build:**\n",
        "1. Load text documents from `./data/raw_text/`\n",
        "2. Chunk them (fixed length with overlap)\n",
        "3. Vectorize using scikit-learn TF-IDF\n",
        "4. Retrieve top-k chunks by cosine similarity\n",
        "5. Generate answers using an LLM with retrieved context\n",
        "\n",
        "**Persistence:**\n",
        "- Vectorizer, matrix, and chunk metadata ‚Üí `./artifacts/naive_tfidf/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Working directory: /Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03\n",
            "‚úÖ Config loaded:\n",
            "  LLM: openrouter (openai/gpt-4o-mini)\n",
            "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
            "  Temperature: 0.2\n",
            "  Artifacts: ./artifacts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03/src/services/llm_services.py:338: UserWarning: ‚ö†Ô∏è  GROQ_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03/src/services/llm_services.py:338: UserWarning: ‚ö†Ô∏è  GOOGLE_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Live Classes/Week 03/src/services/llm_services.py:338: UserWarning: ‚ö†Ô∏è  COHERE_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n"
          ]
        }
      ],
      "source": [
        "#  Global Config & Services (using centralized modules)\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add parent directory to path and change to project root\n",
        "import os\n",
        "\n",
        "# Get the notebook's current directory and find project root\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == \"notebooks\":\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "# Change to project root and add to path\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\" Working directory: {os.getcwd()}\")\n",
        "\n",
        "from src.services.llm_services import (\n",
        "    load_config,\n",
        "    get_llm,\n",
        "    validate_api_keys,\n",
        "    print_config_summary\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Load configuration from config.yaml (now we're in project root)\n",
        "config = load_config(\"src/config/config.yaml\")\n",
        "\n",
        "# Validate API keys\n",
        "validate_api_keys(config, verbose=True)\n",
        "\n",
        "# Print summary\n",
        "print_config_summary(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM initialized: openrouter / gpt-4o-mini\n",
            "\n",
            "üîç Testing API connection...\n",
            "‚úÖ API key verified: API working!\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM using factory from llm_services\n",
        "llm = get_llm(config)\n",
        "print(f\" LLM initialized: {config['llm_provider']} / {config['llm_model']}\")\n",
        "\n",
        "# Verify API key with test completion\n",
        "print(\"\\n Testing API connection...\")\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
        "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
        "    print(f\" API key verified: {test_msg[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\" API key test failed: {e}\")\n",
        "    print(\"  Please check your .env file and API key configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1: Load Text Documents\n",
        "\n",
        "We'll load all `.txt` files from `./data/raw_text/`. If the folder is empty, we'll create sample documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 2 documents\n",
            "  - Understanding Skin Diseases.txt: 6019 chars\n",
            "  - skin-care habits.txt: 6585 chars\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "text_dir = Path(config[\"data_root\"]) / \"raw_text\"\n",
        "text_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load documents\n",
        "doc_files = list(text_dir.glob(\"*.txt\"))\n",
        "\n",
        "if len(doc_files) == 0:\n",
        "    print(\"  No .txt files found. Creating sample documents...\")\n",
        "    \n",
        "    samples = [\n",
        "        (\"doc1.txt\", \"\"\"Eczema, also called atopic dermatitis, is a chronic inflammatory skin condition marked by itchy, dry, and red skin. \n",
        "It affects 10-20% of children and often has a genetic component involving skin barrier dysfunction. Common triggers include allergens, \n",
        "irritants, stress, and environmental factors. Treatment focuses on moisturizing, avoiding triggers, and using topical anti-inflammatory \n",
        "medications when needed.\"\"\"),\n",
        "        \n",
        "        (\"doc2.txt\", \"\"\"Psoriasis is an autoimmune skin condition causing thick, silvery scales and red plaques due to rapid skin cell turnover. \n",
        "It can affect the scalp, elbows, knees, and other areas. Treatment options include topical corticosteroids, calcipotriol ointment, \n",
        "phototherapy, and systemic medications for moderate to severe cases. Triggers may include stress, infections, and certain medications.\"\"\"),\n",
        "        \n",
        "        (\"doc3.txt\", \"\"\"Fungal skin infections like ringworm (tinea) cause circular, red, scaly patches on the skin. They are treated with \n",
        "topical antifungal creams such as terbinafine, or oral antifungals for severe cases. It's important to keep affected areas dry and clean. \n",
        "Athlete's foot and jock itch are common forms of tinea infection affecting feet and groin respectively.\"\"\"),\n",
        "    ]\n",
        "    \n",
        "    for fname, content in samples:\n",
        "        (text_dir / fname).write_text(content)\n",
        "    \n",
        "    doc_files = list(text_dir.glob(\"*.txt\"))\n",
        "\n",
        "documents = []\n",
        "for fpath in doc_files:\n",
        "    documents.append({\n",
        "        \"source\": fpath.name,\n",
        "        \"content\": fpath.read_text(encoding=\"utf-8\")\n",
        "    })\n",
        "\n",
        "print(f\" Loaded {len(documents)} documents\")\n",
        "for doc in documents:\n",
        "    print(f\"  - {doc['source']}: {len(doc['content'])} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Chunk Documents\n",
        "\n",
        "We'll split documents into fixed-size chunks (~800 chars) with overlap (~150 chars) for context continuity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 21 chunks\n",
            "\n",
            "Example chunk:\n",
            "  ID: 0\n",
            "  Source: Understanding Skin Diseases.txt\n",
            "  Text: Sure ‚Äî here‚Äôs a detailed and comprehensive overview of skin diseases, written in an informative, med...\n"
          ]
        }
      ],
      "source": [
        "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 150) -> list:\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to chunk\n",
        "        chunk_size: Maximum size of each chunk in characters\n",
        "        overlap: Number of characters to overlap between chunks\n",
        "        \n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = [] \n",
        "    start = 0 \n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "        start += (chunk_size - overlap)\n",
        "    return chunks\n",
        "        \n",
        "# Create chunks with metadata\n",
        "all_chunks = []\n",
        "for doc in documents:\n",
        "    text_chunks = chunk_text(doc[\"content\"])\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        all_chunks.append({\n",
        "            \"chunk_id\": len(all_chunks),\n",
        "            \"source\": doc[\"source\"],\n",
        "            \"chunk_idx\": i,\n",
        "            \"text\": chunk.strip()\n",
        "        })\n",
        "\n",
        "print(f\" Created {len(all_chunks)} chunks\")\n",
        "print(f\"\\nExample chunk:\")\n",
        "print(f\"  ID: {all_chunks[0]['chunk_id']}\")\n",
        "print(f\"  Source: {all_chunks[0]['source']}\")\n",
        "print(f\"  Text: {all_chunks[0]['text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: TF-IDF Vectorization\n",
        "\n",
        "We'll use scikit-learn's `TfidfVectorizer` with 1-2 n-grams to capture both individual terms and bigrams.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TF-IDF matrix shape: (21, 1924)\n",
            "  Chunks: 21\n",
            "  Vocabulary size: 1924\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import save_npz, load_npz\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "                            ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "                            max_df=0.85,         # Ignore terms in >85% of documents\n",
        "                            min_df=1,            # Must appear in at least 1 document\n",
        "                            lowercase=True,\n",
        "                            stop_words='english'\n",
        "                            )\n",
        "\n",
        "# Fit and transform\n",
        "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
        "tfidf_matrix = vectorizer.fit_transform(chunk_texts)\n",
        "\n",
        "print(f\" TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "print(f\"  Chunks: {tfidf_matrix.shape[0]}\")\n",
        "print(f\"  Vocabulary size: {tfidf_matrix.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Persist Artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Artifacts saved to artifacts/naive_tfidf\n",
            "  - vectorizer.pkl\n",
            "  - matrix.npz\n",
            "  - chunks.parquet\n"
          ]
        }
      ],
      "source": [
        "naive_dir = Path(config[\"artifacts_root\"]) / \"naive_tfidf\"\n",
        "naive_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save vectorizer\n",
        "with open(naive_dir / \"vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "# Save TF-IDF matrix\n",
        "save_npz(naive_dir / \"matrix.npz\", tfidf_matrix)\n",
        "\n",
        "# Save chunk metadata\n",
        "chunks_df = pd.DataFrame(all_chunks)\n",
        "chunks_df.to_parquet(naive_dir / \"chunks.parquet\", index=False)\n",
        "\n",
        "print(f\" Artifacts saved to {naive_dir}\")\n",
        "print(f\"  - vectorizer.pkl\")\n",
        "print(f\"  - matrix.npz\")\n",
        "print(f\"  - chunks.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Retrieval (Cosine Similarity)\n",
        "\n",
        "Given a query, we:\n",
        "1. Transform it using the same vectorizer\n",
        "2. Compute cosine similarity with all chunks\n",
        "3. Return top-k most similar chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is eczema and how is it treated?\n",
            "\n",
            "Result 1 (score: 0.068):\n",
            "  Source: skin-care habits.txt\n",
            "  Text: ctive dressings and avoiding skin-to-skin spread; wart options include salicylic acid preparations or cryotherapy admini...\n",
            "\n",
            "Result 2 (score: 0.068):\n",
            "  Source: Understanding Skin Diseases.txt\n",
            "  Text: PV).\n",
            "\t‚Ä¢\tFungal infections\n",
            "\t‚Ä¢\tRingworm (Tinea): Circular, red, scaly patches.\n",
            "\t‚Ä¢\tCandidiasis: Yeast infection affecting m...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def retrieve_chunks(query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Retrieve top-k most relevant chunks for a query using TF-IDF and cosine similarity.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query string\n",
        "        top_k: Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries containing chunk data and similarity scores\n",
        "    \"\"\"\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    \n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "                        **all_chunks[idx],\n",
        "                        \"score\": float(similarities[idx])\n",
        "                        })\n",
        "    return results\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"What is eczema and how is it treated?\"\n",
        "retrieved = retrieve_chunks(test_query, top_k=2)\n",
        "\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "for i, chunk in enumerate(retrieved, 1):\n",
        "    print(f\"Result {i} (score: {chunk['score']:.3f}):\")\n",
        "    print(f\"  Source: {chunk['source']}\")\n",
        "    print(f\"  Text: {chunk['text'][:120]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: RAG Pipeline (Retrieve + Generate)\n",
        "\n",
        "Now we'll combine retrieval with LLM generation:\n",
        "1. Retrieve top-k chunks\n",
        "2. Build a prompt with the retrieved context\n",
        "3. Generate an answer using the LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAG pipeline ready\n"
          ]
        }
      ],
      "source": [
        "def build_rag_prompt(query: str, retrieved_chunks: list) -> str:\n",
        "    \"\"\"\n",
        "    Build a RAG prompt with retrieved context.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        retrieved_chunks: List of retrieved chunk dictionaries\n",
        "        \n",
        "    Returns:\n",
        "        Complete prompt string with context and question\n",
        "    \"\"\"\n",
        "    context_parts = []\n",
        "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "        context_parts.append(f\"[{i}] {chunk['text']}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"You are a concise assitant for healthcare. Use only the provided context to answer the question.\n",
        "    Keep answers under 5 sentences. Based on below conditions adapt your answer.\n",
        "\n",
        "    1. If question is about Skin diseases and the infomation is insufficent, say \"I do not have info, please reach to our hospital\".\n",
        "    2. If question is iirelevant to Skin diseases, say \"I can't provide info\"\n",
        "    \n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "    \n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def naive_rag(query: str, top_k: int = 3) -> dict:\n",
        "    \"\"\"Complete RAG pipeline: retrieve + generate.\"\"\"\n",
        "    # Retrieve\n",
        "    retrieved = retrieve_chunks(query, top_k=top_k)\n",
        "    \n",
        "    # Build prompt\n",
        "    prompt = build_rag_prompt(query, retrieved)\n",
        "\n",
        "    # Generate\n",
        "    response = llm.invoke(prompt)\n",
        "    answer = response.content if hasattr(response, 'content') else str(response)\n",
        "    \n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"retrieved_chunks\": retrieved,\n",
        "        \"num_chunks\": len(retrieved)\n",
        "    }\n",
        "\n",
        "print(\" RAG pipeline ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Interactive RAG Demo\n",
        "\n",
        "Ask questions and see the retrieved chunks + generated answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "‚ùì Query: What is eczema and how is it treated?\n",
            "\n",
            "üìö Retrieved chunks:\n",
            "  [1] skin-care habits.txt (score: 0.068)\n",
            "      ctive dressings and avoiding skin-to-skin spread; wart options include salicylic acid preparations o...\n",
            "\n",
            "  [2] Understanding Skin Diseases.txt (score: 0.068)\n",
            "      PV).\n",
            "\t‚Ä¢\tFungal infections\n",
            "\t‚Ä¢\tRingworm (Tinea): Circular, red, scaly patches.\n",
            "\t‚Ä¢\tCandidiasis: Yeast i...\n",
            "\n",
            "üí° Answer:\n",
            "Eczema, or atopic dermatitis, is a chronic inflammatory disease characterized by itchy, dry, red skin. Treatment includes daily emollients, short lukewarm baths, fragrance-free products, and avoiding triggers like wool and harsh detergents. During flare-ups, clinicians may prescribe topical anti-inflammatories, wet-wraps, and selected phototherapy for moderate to severe cases. Additionally, dilute bleach baths may be recommended by a dermatologist to reduce Staph burden and itch.\n",
            "\n",
            "================================================================================\n",
            "‚ùì Query: How do you treat fungal skin infections?\n",
            "\n",
            "üìö Retrieved chunks:\n",
            "  [1] Understanding Skin Diseases.txt (score: 0.134)\n",
            "      n be broadly categorized into several groups based on their causes and characteristics:\n",
            "\n",
            "1. Infectio...\n",
            "\n",
            "  [2] Understanding Skin Diseases.txt (score: 0.116)\n",
            "      PV).\n",
            "\t‚Ä¢\tFungal infections\n",
            "\t‚Ä¢\tRingworm (Tinea): Circular, red, scaly patches.\n",
            "\t‚Ä¢\tCandidiasis: Yeast i...\n",
            "\n",
            "üí° Answer:\n",
            "I do not have info, please reach to our hospital.\n",
            "\n",
            "================================================================================\n",
            "‚ùì Query: What are the symptoms of psoriasis?\n",
            "\n",
            "üìö Retrieved chunks:\n",
            "  [1] Understanding Skin Diseases.txt (score: 0.104)\n",
            "      uction.\n",
            "\t‚Ä¢\tPost-inflammatory hyperpigmentation: Darkening of skin following injury or inflammation.\n",
            "...\n",
            "\n",
            "  [2] skin-care habits.txt (score: 0.053)\n",
            "      e.  Ôøº\n",
            "\n",
            "‚∏ª\n",
            "\n",
            "What not to do (general)\n",
            "\t‚Ä¢\tDon‚Äôt apply neat bleach, vinegar, or essential oils to open/ir...\n",
            "\n",
            "üí° Answer:\n",
            "I do not have info, please reach to our hospital.\n",
            "\n",
            "================================================================================\n",
            "‚ùì Query: Who won ashes 2025 ?\n",
            "\n",
            "üìö Retrieved chunks:\n",
            "  [1] skin-care habits.txt (score: 0.000)\n",
            "      intness.  Ôøº\n",
            "\t‚Ä¢\tA changing mole (ABCDE features) or a non-healing, bleeding lesion.  Ôøº...\n",
            "\n",
            "  [2] Understanding Skin Diseases.txt (score: 0.000)\n",
            "      discrimination\n",
            "\n",
            "Thus, dermatological care must include psychological support and public awareness ca...\n",
            "\n",
            "üí° Answer:\n",
            "I can't provide info.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example queries about dermatology\n",
        "queries = [\n",
        "    \"What is eczema and how is it treated?\",\n",
        "    \"How do you treat fungal skin infections?\",\n",
        "    \"What are the symptoms of psoriasis?\",\n",
        "    \"Who won ashes 2025 ?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\" Query: {query}\\n\")\n",
        "    \n",
        "    result = naive_rag(query, top_k=2)\n",
        "    \n",
        "    print(\" Retrieved chunks:\")\n",
        "    for i, chunk in enumerate(result[\"retrieved_chunks\"], 1):\n",
        "        print(f\"  [{i}] {chunk['source']} (score: {chunk['score']:.3f})\")\n",
        "        print(f\"      {chunk['text'][:100]}...\\n\")\n",
        "    \n",
        "    print(f\" Answer:\\n{result['answer']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**What we learned:**\n",
        "- ‚úÖ Basic RAG workflow: chunk ‚Üí vectorize ‚Üí retrieve ‚Üí generate\n",
        "- ‚úÖ TF-IDF captures term importance within and across documents\n",
        "- ‚úÖ Cosine similarity measures semantic relevance\n",
        "- ‚úÖ LLM uses retrieved context to ground its answers\n",
        "\n",
        "**Limitations of this approach:**\n",
        "- TF-IDF is sparse and keyword-based (misses semantic similarity)\n",
        "- No understanding of context or synonyms\n",
        "- Fixed chunk size doesn't respect document structure\n",
        "\n",
        "**Artifacts persisted:**\n",
        "- `./artifacts/naive_tfidf/vectorizer.pkl`\n",
        "- `./artifacts/naive_tfidf/matrix.npz`\n",
        "- `./artifacts/naive_tfidf/chunks.parquet`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sahas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
