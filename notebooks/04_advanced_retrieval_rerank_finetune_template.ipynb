{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4 – Advanced Retrieval: Hybrid, Reranking, Finetuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Working directory: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Week 03\n",
            " Config loaded:\n",
            "  LLM: groq / openai/gpt-oss-120b\n",
            "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
            "  Temperature: 0.2\n",
            "  Artifacts: ./artifacts\n"
          ]
        }
      ],
      "source": [
        "#  Global Config & Services (using centralized modules)\n",
        "\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add parent directory to path and change to project root\n",
        "import os\n",
        "\n",
        "# Get the notebook's current directory and find project root\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == \"notebooks\":\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "# Change to project root and add to path\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\" Working directory: {os.getcwd()}\")\n",
        "\n",
        "from src.services.llm_services import (\n",
        "    load_config,\n",
        "    get_llm,\n",
        "    get_text_embeddings,\n",
        "    validate_api_keys,\n",
        "    print_config_summary\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Load configuration from config.yaml (now we're in project root)\n",
        "config = load_config(\"src/config/config.yaml\")\n",
        "\n",
        "# Validate API keys\n",
        "validate_api_keys(config, verbose=True)\n",
        "\n",
        "# Print summary\n",
        "print_config_summary(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Week 03\\src\\services\\llm_services.py:129: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  return HuggingFaceEmbeddings(\n",
            "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "567caf2916754afeb1c89b6b9f0ad4c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sahas Induwara\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72f69c54f8b34efd89637fead555acd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0dba62244c44236a2ba3e442b67c985",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b6bb7e84d1c477a87469a848d119592",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "907c60561c9e449bb4a009b12232a2c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "015841bf94594db5ba949afcae932425",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "135549fa7e6543e693ab2b525ae74c2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " LLM: groq / gpt-4o-mini\n",
            " Embeddings: sentence-transformers/all-MiniLM-L6-v2\n",
            " Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "\n",
            " Testing LLM API connection...\n",
            " LLM API verified: API working!\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM, Embeddings, and Reranker\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "llm = get_llm(config)\n",
        "embeddings = get_text_embeddings(config)\n",
        "\n",
        "# Reranker (specific to this notebook)\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "print(f\" LLM: {config['llm_provider']} / {config.get('openrouter_model', config.get('llm_model'))}\")\n",
        "print(f\" Embeddings: {config['text_emb_model']}\")\n",
        "print(f\" Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# Verify API key with test completion\n",
        "print(\"\\n Testing LLM API connection...\")\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
        "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
        "    print(f\" LLM API verified: {test_msg[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\" LLM API test failed: {e}\")\n",
        "    print(\"  Please check your .env file and API key configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1: Load or Create Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading dermatology corpus from raw_text files...\n",
            "  Loading: skin-care habits.txt\n",
            "  Loading: Understanding Skin Diseases.txt\n",
            " Loaded 34 dermatology documents from text files\n",
            "  Average length: 348 chars\n",
            "  Topics: eczema, psoriasis, fungal infections, treatments\n",
            "\n",
            "Sample: Foundations: skin-care habits that help most conditions • Gentle cleansing: fragrance-free cleansers; avoid harsh scrubs...\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "import re\n",
        "\n",
        "# Load corpus dynamically from raw_text files\n",
        "text_dir = Path(config[\"data_root\"]) / \"raw_text\"\n",
        "\n",
        "def load_and_chunk_text_files(directory: Path, chunk_size: int = 500):\n",
        "    \"\"\"Load text files and chunk them into manageable paragraphs.\"\"\"\n",
        "    corpus = []\n",
        "    \n",
        "    for txt_file in directory.glob(\"*.txt\"):\n",
        "        print(f\"  Loading: {txt_file.name}\")\n",
        "        content = txt_file.read_text(encoding='utf-8')\n",
        "        \n",
        "        # Split by double newlines (paragraphs) or section markers\n",
        "        paragraphs = re.split(r'\\n\\n+|⸻', content)\n",
        "        \n",
        "        for para in paragraphs:\n",
        "            # Clean and normalize\n",
        "            para = para.strip()\n",
        "            \n",
        "            # Skip very short paragraphs, headers, or empty lines\n",
        "            if len(para) < 50 or para.startswith('•') or para.startswith('#'):\n",
        "                continue\n",
        "            \n",
        "            # Remove excessive whitespace and bullet points\n",
        "            para = re.sub(r'\\s+', ' ', para)\n",
        "            para = re.sub(r'^\\s*[•\\-]\\s*', '', para)\n",
        "            \n",
        "            # Skip if still too short after cleaning\n",
        "            if len(para) < 100:\n",
        "                continue\n",
        "                \n",
        "            corpus.append(para)\n",
        "    \n",
        "    return corpus\n",
        "\n",
        "print(\" Loading dermatology corpus from raw_text files...\")\n",
        "corpus = load_and_chunk_text_files(text_dir)\n",
        "\n",
        "# Create documents with metadata\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=text, \n",
        "        metadata={\n",
        "            'doc_id': i, \n",
        "            'source': 'dermatology_corpus',\n",
        "            'length': len(text)\n",
        "        }\n",
        "    ) \n",
        "    for i, text in enumerate(corpus)\n",
        "]\n",
        "\n",
        "print(f\" Loaded {len(documents)} dermatology documents from text files\")\n",
        "\n",
        "# Check if loaded successfully - add fallback if empty\n",
        "if len(documents) == 0:\n",
        "    print(\"  No documents from raw_text. Creating sample corpus...\")\n",
        "    sample_texts = [\n",
        "        \"Eczema (atopic dermatitis) is a chronic inflammatory skin condition. Treatment includes daily moisturizing, topical corticosteroids during flare-ups, and avoiding triggers.\",\n",
        "        \"Psoriasis is an autoimmune condition causing rapid skin cell turnover, resulting in thick, silvery scales. Common treatments include topical corticosteroids, phototherapy, and systemic medications.\",\n",
        "        \"Fungal infections (tinea) such as ringworm are caused by dermatophytes. Treatment involves topical antifungal creams like terbinafine applied for 2-4 weeks.\",\n",
        "        \"Acne vulgaris occurs when hair follicles become clogged. Treatment options include topical retinoids, benzoyl peroxide, and oral antibiotics for severe cases.\",\n",
        "        \"Contact dermatitis results from skin exposure to irritants or allergens. Management involves identifying and avoiding triggers.\",\n",
        "        \"Rosacea causes facial redness and visible blood vessels. Treatment includes avoiding triggers and topical medications like metronidazole.\",\n",
        "        \"Seborrheic dermatitis causes scaly patches on the scalp. Treatment includes medicated shampoos containing ketoconazole.\",\n",
        "        \"Vitiligo causes loss of skin pigmentation. Management includes sun protection, topical corticosteroids, and phototherapy.\",\n",
        "    ]\n",
        "    documents = [\n",
        "        Document(page_content=text, metadata={\"doc_id\": i, \"source\": \"sample_corpus\", \"length\": len(text)})\n",
        "        for i, text in enumerate(sample_texts)\n",
        "    ]\n",
        "    print(f\" Created {len(documents)} sample documents\")\n",
        "\n",
        "if len(documents) > 0:\n",
        "    avg_len = sum(len(d.page_content) for d in documents) // len(documents)\n",
        "    print(f\"  Average length: {avg_len} chars\")\n",
        "    print(f\"  Topics: eczema, psoriasis, fungal infections, treatments\")\n",
        "    print(f\"\\nSample: {documents[0].page_content[:120]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Dense Retrieval (ChromaDB)\n",
        "\n",
        "Build a vector store using dense embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Building dense vector store...\n",
            " Dense index built: 34 docs\n",
            "\n",
            " Dense search: 'What are treatments for eczema?'\n",
            "  [1] Eczema (atopic dermatitis) • Core remedies: daily emollients, short lukewarm baths/showers, fragranc...\n",
            "  [2] Treatment depends on the underlying cause and may include: • Topical medications: Corticosteroids, a...\n",
            "  [3] Melasma & post-inflammatory hyperpigmentation • Essential: daily SPF 30+, hats/UPF clothing, and avo...\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "chroma_root = Path(config[\"artifacts_root\"]) / \"chroma\"\n",
        "chroma_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\" Building dense vector store...\")\n",
        "\n",
        "dense_vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"advanced_dense\",\n",
        "    persist_directory=str(chroma_root / \"advanced_dense\"),\n",
        ")\n",
        "\n",
        "print(f\" Dense index built: {len(documents)} docs\")\n",
        "\n",
        "# Test dense retrieval\n",
        "query = \"What are treatments for eczema?\"\n",
        "dense_results = dense_vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"\\n Dense search: '{query}'\")\n",
        "for i, doc in enumerate(dense_results, 1):\n",
        "    print(f\"  [{i}] {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Sparse Retrieval (BM25)\n",
        "\n",
        "Use BM25 for keyword-based retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Building BM25 index...\n",
            " BM25 index built\n",
            "\n",
            " BM25 search: 'What are treatments for eczema?'\n",
            "  [1] (score: 3.76) Actinic keratoses, BCC, SCC, melanoma • What helps: prevention & early detection. Follow the ABCDE s...\n",
            "  [2] (score: 2.61) Urticaria (hives) • What helps: for most, second-generation oral antihistamines (non-sedating) are f...\n",
            "  [3] (score: 2.21) 2) Fungal (tinea/ringworm, athlete’s foot, jock itch) • What helps at home: OTC antifungals (creams,...\n"
          ]
        }
      ],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "print(\" Building BM25 index...\")\n",
        "\n",
        "# Tokenize corpus\n",
        "tokenized_corpus = [doc.page_content.lower().split() for doc in documents]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "print(f\" BM25 index built\")\n",
        "\n",
        "def bm25_search(query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Search using BM25 (sparse retrieval algorithm).\n",
        "    \n",
        "    Args:\n",
        "        query: Search query string\n",
        "        top_k: Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries with doc, score, and doc_id\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    tokenized_query = query.lower().split()\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            \"doc\": documents[idx],\n",
        "            \"score\": float(scores[idx]),\n",
        "            \"doc_id\": idx\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Test BM25\n",
        "bm25_results = bm25_search(query, top_k=3)\n",
        "\n",
        "print(f\"\\n BM25 search: '{query}'\")\n",
        "for i, res in enumerate(bm25_results, 1):\n",
        "    print(f\"  [{i}] (score: {res['score']:.2f}) {res['doc'].page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Hybrid Fusion (Dense + BM25)\n",
        "\n",
        "Combine dense and sparse retrieval using Reciprocal Rank Fusion (RRF).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hybrid (RRF) search: 'What are treatments for eczema?'\n",
            "  [1] (RRF: 0.016) Eczema (atopic dermatitis) • Core remedies: daily emollients, short lukewarm baths/showers, fragranc...\n",
            "  [2] (RRF: 0.016) Actinic keratoses, BCC, SCC, melanoma • What helps: prevention & early detection. Follow the ABCDE s...\n",
            "  [3] (RRF: 0.016) Treatment depends on the underlying cause and may include: • Topical medications: Corticosteroids, a...\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "def rrf_fusion(dense_docs: List, bm25_results: List, k: int = 60) -> List:\n",
        "    \"\"\"\n",
        "    Reciprocal Rank Fusion - combines dense and sparse retrieval.\n",
        "    \n",
        "    Args:\n",
        "        dense_docs: Results from dense (vector) retrieval\n",
        "        bm25_results: Results from BM25 (sparse) retrieval\n",
        "        k: Constant for RRF formula (default 60)\n",
        "        \n",
        "    Returns:\n",
        "        Fused results sorted by RRF score\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Initialize an empty dictionary: rrf_scores = {}\n",
        "    #\n",
        "    # 2. Add dense scores:\n",
        "    #    - Loop through dense_docs with enumerate(dense_docs, 1) to get rank and doc\n",
        "    #    - Get doc_id from doc.metadata[\"doc_id\"]\n",
        "    #    - Calculate RRF score: 1.0 / (k + rank)\n",
        "    #    - Add to rrf_scores[doc_id] (initialize to 0.0 if not exists)\n",
        "    #    - Use .get(doc_id, 0.0) to handle missing keys\n",
        "    #\n",
        "    # 3. Add BM25 scores:\n",
        "    #    - Loop through bm25_results with enumerate(bm25_results, 1) to get rank and res\n",
        "    #    - Get doc_id from res[\"doc_id\"]\n",
        "    #    - Calculate RRF score: 1.0 / (k + rank)\n",
        "    #    - Add to rrf_scores[doc_id]\n",
        "    #\n",
        "    # 4. Sort by RRF score:\n",
        "    #    - Use sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    #    - This gives list of (doc_id, score) tuples sorted by score\n",
        "    #\n",
        "    # 5. Build results list:\n",
        "    #    - Loop through sorted_ids\n",
        "    #    - For each (doc_id, score), append dictionary with:\n",
        "    #      - \"doc\": documents[doc_id]\n",
        "    #      - \"score\": score\n",
        "    #      - \"doc_id\": doc_id\n",
        "    #\n",
        "    # 6. Return fused_docs list\n",
        "    \n",
        "    rrf_scores = {}\n",
        "\n",
        "    for rank, doc in enumerate(dense_docs, 1):\n",
        "        doc_id = doc.metadata[\"doc_id\"]\n",
        "        rrf_score = 1.0 / (k + rank)\n",
        "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0.0) + rrf_score\n",
        "\n",
        "    for rank, res in enumerate(bm25_results, 1):\n",
        "        doc_id = res[\"doc_id\"]\n",
        "        rrf_score = 1.0 / (k + rank)\n",
        "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0.0) + rrf_score\n",
        "\n",
        "    # Sort by RRF score descending\n",
        "    sorted_ids = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    fused_docs = []\n",
        "    for doc_id, score in sorted_ids:\n",
        "        fused_docs.append({\n",
        "            \"doc\": documents[doc_id],\n",
        "            \"score\": score,\n",
        "            \"doc_id\": doc_id\n",
        "        })\n",
        "\n",
        "    return fused_docs\n",
        "\n",
        "# Test hybrid fusion\n",
        "fused_results = rrf_fusion(dense_results, bm25_results)[:3]\n",
        "\n",
        "print(f\" Hybrid (RRF) search: '{query}'\")\n",
        "for i, res in enumerate(fused_results, 1):\n",
        "    print(f\"  [{i}] (RRF: {res['score']:.3f}) {res['doc'].page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Reranking with Cross-Encoder\n",
        "\n",
        "Refine results using a cross-encoder for more accurate relevance scoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Reranked results: 'What are treatments for eczema?'\n",
            "  [1] (rerank: 4.700) Eczema (atopic dermatitis) • Core remedies: daily emollients, short lukewarm baths/showers, fragranc...\n",
            "  [2] (rerank: -1.330) Treatment depends on the underlying cause and may include: • Topical medications: Corticosteroids, a...\n",
            "  [3] (rerank: -1.724) Actinic keratoses, BCC, SCC, melanoma • What helps: prevention & early detection. Follow the ABCDE s...\n"
          ]
        }
      ],
      "source": [
        "def rerank(query: str, results: List, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Rerank results using a cross-encoder for more accurate relevance scoring.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query\n",
        "        results: List of initial results to rerank\n",
        "        top_k: Number of top results to return after reranking\n",
        "        \n",
        "    Returns:\n",
        "        Reranked results with rerank_score added\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Prepare query-document pairs for the cross-encoder:\n",
        "    #    - Create list of [query, document_content] pairs\n",
        "    #    - Use list comprehension: [[query, res[\"doc\"].page_content] for res in results]\n",
        "    #    - Store in variable: pairs\n",
        "    #\n",
        "    # 2. Get scores from the cross-encoder:\n",
        "    #    - Use: reranker.predict(pairs)\n",
        "    #    - This returns array of relevance scores\n",
        "    #\n",
        "    # 3. Add rerank scores to results:\n",
        "    #    - Loop through results with enumerate to get index i and result res\n",
        "    #    - Add new field: res[\"rerank_score\"] = float(scores[i])\n",
        "    #\n",
        "    # 4. Sort by rerank_score:\n",
        "    #    - Use: sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "    #    - Take top_k: [:top_k]\n",
        "    #\n",
        "    # 5. Return the reranked results\n",
        "    \n",
        "    pairs = [[query, res[\"doc\"].page_content] for res in results]\n",
        "    scores = reranker.predict(pairs)\n",
        "\n",
        "    for i, res in enumerate(results):\n",
        "        res[\"rerank_score\"] = float(scores[i])\n",
        "\n",
        "    reranked = sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)[:top_k]\n",
        "    return reranked\n",
        "\n",
        "# Test reranking\n",
        "reranked_results = rerank(query, fused_results[:6], top_k=3)\n",
        "\n",
        "print(f\" Reranked results: '{query}'\")\n",
        "for i, res in enumerate(reranked_results, 1):\n",
        "    print(f\"  [{i}] (rerank: {res['rerank_score']:.3f}) {res['doc'].page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bonus Exercise: Complete Hybrid RAG Pipeline\n",
        "\n",
        "**Challenge:** Combine all techniques into a single end-to-end pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_rag_pipeline(query: str, dense_top_n: int = 10, bm25_top_n: int = 10, rerank_top_n: int = 6, final_top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Complete hybrid RAG pipeline combining all advanced techniques.\n",
        "    \n",
        "    Pipeline: Dense Retrieval → BM25 Retrieval → RRF Fusion → Reranking → LLM Generation\n",
        "    \n",
        "    Args:\n",
        "        query: User question\n",
        "        dense_top_n: Number of results from dense retrieval\n",
        "        bm25_top_n: Number of results from BM25\n",
        "        rerank_top_n: Number of fused results to rerank\n",
        "        final_top_k: Final number of chunks to use for generation\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with query, answer, and retrieved_docs\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 25-30 lines)\n",
        "    # YOUR CODE HERE\n",
        "    # HINTS:\n",
        "    # 1. Dense retrieval:\n",
        "    #    - Use: dense_vectorstore.similarity_search(query, k=dense_top_n)\n",
        "    #    - Store in: dense_results\n",
        "    #\n",
        "    # 2. Sparse (BM25) retrieval:\n",
        "    #    - Use: bm25_search(query, top_k=bm25_top_n)\n",
        "    #    - Store in: bm25_results\n",
        "    #\n",
        "    # 3. Fusion:\n",
        "    #    - Use: rrf_fusion(dense_results, bm25_results)\n",
        "    #    - Take top rerank_top_n: [:rerank_top_n]\n",
        "    #    - Store in: fused_results\n",
        "    #\n",
        "    # 4. Reranking:\n",
        "    #    - Use: rerank(query, fused_results, top_k=final_top_k)\n",
        "    #    - Store in: reranked_results\n",
        "    #\n",
        "    # 5. Build context from reranked results:\n",
        "    #    - Extract text from each result: res[\"doc\"].page_content\n",
        "    #    - Join with \"\\n\\n\"\n",
        "    #    - Store in: context\n",
        "    #\n",
        "    # 6. Build RAG prompt:\n",
        "    #    - System instruction + context + question\n",
        "    #    - Example: f\"\"\"Use the context to answer the question...\n",
        "    #                  Context: {context}\n",
        "    #                  Question: {query}\n",
        "    #                  Answer:\"\"\"\n",
        "    #\n",
        "    # 7. Generate answer:\n",
        "    #    - Use: llm.invoke(prompt)\n",
        "    #    - Extract text: response.content if hasattr(response, 'content') else str(response)\n",
        "    #\n",
        "    # 8. Return dictionary with:\n",
        "    #    - \"query\": query\n",
        "    #    - \"answer\": answer\n",
        "    #    - \"retrieved_docs\": reranked_results\n",
        "    #    - \"num_dense\": len(dense_results)\n",
        "    #    - \"num_bm25\": len(bm25_results)\n",
        "    #    - \"num_fused\": len(fused_results)\n",
        "    #    - \"num_final\": len(reranked_results)\n",
        "    \n",
        "    raise NotImplementedError(\"Complete the hybrid_rag_pipeline function\")\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "# Test the complete pipeline\n",
        "print(\" Testing Complete Hybrid RAG Pipeline\\n\")\n",
        "test_query = \"What are treatments for eczema?\"\n",
        "result = hybrid_rag_pipeline(test_query)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"\\nPipeline stats:\")\n",
        "print(f\"  Dense retrieval: {result.get('num_dense', 0)} docs\")\n",
        "print(f\"  BM25 retrieval: {result.get('num_bm25', 0)} docs\")\n",
        "print(f\"  After fusion: {result.get('num_fused', 0)} docs\")\n",
        "print(f\"  After reranking: {result.get('num_final', 0)} docs\")\n",
        "print(f\"\\nFinal Answer:\\n{result['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**What we covered:**\n",
        "\n",
        "✅ **Dense Retrieval** - Vector similarity search with ChromaDB  \n",
        "✅ **Sparse Retrieval** - BM25 keyword matching  \n",
        "✅ **Hybrid Fusion** - Reciprocal Rank Fusion (RRF) combining both  \n",
        "✅ **Reranking** - Cross-encoder for refined relevance scoring  \n",
        "✅ **Dynamic Loading** - Corpus loaded from raw_text files  \n",
        "\n",
        "**Retrieval Pipeline:**\n",
        "```\n",
        "Query → Dense (top-N) + BM25 (top-N) → Fusion (RRF) → Rerank (cross-encoder) → Final top-k\n",
        "```\n",
        "\n",
        "**When to use each:**\n",
        "- **BM25**: Keyword/exact match queries\n",
        "- **Dense**: Semantic/paraphrase queries  \n",
        "- **Hybrid**: Best of both worlds\n",
        "- **Reranking**: Highest precision (slower but more accurate)\n",
        "\n",
        "**Trade-offs:**\n",
        "- Hybrid + reranking: Better accuracy, higher latency\n",
        "- Dense only: Fast, good for semantic search\n",
        "- BM25 only: Fast, good for keyword search\n",
        "\n",
        "**Artifacts:**\n",
        "- `./artifacts/chroma/advanced_dense/`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sahas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
