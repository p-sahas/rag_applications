{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4 ‚Äì Advanced Retrieval: Hybrid, Reranking, Finetuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Config loaded:\n",
            "  LLM: openrouter (openai/gpt-4o-mini)\n",
            "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
            "  Temperature: 0.2\n",
            "  Artifacts: ./artifacts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Codes/RAG Systems/src/services/llm_services.py:338: UserWarning: ‚ö†Ô∏è  GROQ_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Codes/RAG Systems/src/services/llm_services.py:338: UserWarning: ‚ö†Ô∏è  GOOGLE_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n",
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Codes/RAG Systems/src/services/llm_services.py:338: UserWarning: ‚ö†Ô∏è  COHERE_API_KEY not found in environment\n",
            "  warnings.warn(f\"‚ö†Ô∏è  {key} not found in environment\")\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è Global Config & Services (using centralized modules)\n",
        "\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add parent directory to path and change to project root\n",
        "import os\n",
        "\n",
        "# Get the notebook's current directory and find project root\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == \"notebooks\":\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    project_root = notebook_dir\n",
        "\n",
        "# Change to project root and add to path\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
        "\n",
        "from src.services.llm_services import (\n",
        "    load_config,\n",
        "    get_llm,\n",
        "    get_text_embeddings,\n",
        "    validate_api_keys,\n",
        "    print_config_summary\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Load configuration from config.yaml (now we're in project root)\n",
        "config = load_config(\"src/config/config.yaml\")\n",
        "\n",
        "# Validate API keys\n",
        "validate_api_keys(config, verbose=True)\n",
        "\n",
        "# Print summary\n",
        "print_config_summary(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/machinelearningzuu/Dropbox/Zuu Crew/Courses/üöß AI Engineer Essentials/Codes/RAG Systems/src/services/llm_services.py:129: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  return HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM: openrouter / gpt-4o-mini\n",
            "‚úÖ Embeddings: sentence-transformers/all-MiniLM-L6-v2\n",
            "‚úÖ Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "\n",
            "üîç Testing LLM API connection...\n",
            "‚úÖ LLM API verified: API working!\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM, Embeddings, and Reranker\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "llm = get_llm(config)\n",
        "embeddings = get_text_embeddings(config)\n",
        "\n",
        "# Reranker (specific to this notebook)\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "print(f\"‚úÖ LLM: {config['llm_provider']} / {config.get('openrouter_model', config.get('llm_model'))}\")\n",
        "print(f\"‚úÖ Embeddings: {config['text_emb_model']}\")\n",
        "print(f\"‚úÖ Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# Verify API key with test completion\n",
        "print(\"\\nüîç Testing LLM API connection...\")\n",
        "try:\n",
        "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
        "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
        "    print(f\"‚úÖ LLM API verified: {test_msg[:50]}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå LLM API test failed: {e}\")\n",
        "    print(\"‚ö†Ô∏è  Please check your .env file and API key configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1: Load or Create Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Loading dermatology corpus from raw_text files...\n",
            "  Loading: Understanding Skin Diseases.txt\n",
            "  Loading: skin-care habits.txt\n",
            "‚úÖ Loaded 34 dermatology documents from text files\n",
            "  Average length: 348 chars\n",
            "  Topics: eczema, psoriasis, fungal infections, treatments\n",
            "\n",
            "Sample: Sure ‚Äî here‚Äôs a detailed and comprehensive overview of skin diseases, written in an informative, medically accurate styl...\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema import Document\n",
        "import re\n",
        "\n",
        "# Load corpus dynamically from raw_text files\n",
        "text_dir = Path(config[\"data_root\"]) / \"raw_text\"\n",
        "\n",
        "def load_and_chunk_text_files(directory: Path, chunk_size: int = 500):\n",
        "    \"\"\"Load text files and chunk them into manageable paragraphs.\"\"\"\n",
        "    corpus = []\n",
        "    \n",
        "    for txt_file in directory.glob(\"*.txt\"):\n",
        "        print(f\"  Loading: {txt_file.name}\")\n",
        "        content = txt_file.read_text(encoding='utf-8')\n",
        "        \n",
        "        # Split by double newlines (paragraphs) or section markers\n",
        "        paragraphs = re.split(r'\\n\\n+|‚∏ª', content)\n",
        "        \n",
        "        for para in paragraphs:\n",
        "            # Clean and normalize\n",
        "            para = para.strip()\n",
        "            \n",
        "            # Skip very short paragraphs, headers, or empty lines\n",
        "            if len(para) < 50 or para.startswith('‚Ä¢') or para.startswith('#'):\n",
        "                continue\n",
        "            \n",
        "            # Remove excessive whitespace and bullet points\n",
        "            para = re.sub(r'\\s+', ' ', para)\n",
        "            para = re.sub(r'^\\s*[‚Ä¢\\-]\\s*', '', para)\n",
        "            \n",
        "            # Skip if still too short after cleaning\n",
        "            if len(para) < 100:\n",
        "                continue\n",
        "                \n",
        "            corpus.append(para)\n",
        "    \n",
        "    return corpus\n",
        "\n",
        "print(\"üìö Loading dermatology corpus from raw_text files...\")\n",
        "corpus = load_and_chunk_text_files(text_dir)\n",
        "\n",
        "# Create documents with metadata\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=text, \n",
        "        metadata={\n",
        "            'doc_id': i, \n",
        "            'source': 'dermatology_corpus',\n",
        "            'length': len(text)\n",
        "        }\n",
        "    ) \n",
        "    for i, text in enumerate(corpus)\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(documents)} dermatology documents from text files\")\n",
        "\n",
        "# Check if loaded successfully - add fallback if empty\n",
        "if len(documents) == 0:\n",
        "    print(\"‚ö†Ô∏è  No documents from raw_text. Creating sample corpus...\")\n",
        "    sample_texts = [\n",
        "        \"Eczema (atopic dermatitis) is a chronic inflammatory skin condition. Treatment includes daily moisturizing, topical corticosteroids during flare-ups, and avoiding triggers.\",\n",
        "        \"Psoriasis is an autoimmune condition causing rapid skin cell turnover, resulting in thick, silvery scales. Common treatments include topical corticosteroids, phototherapy, and systemic medications.\",\n",
        "        \"Fungal infections (tinea) such as ringworm are caused by dermatophytes. Treatment involves topical antifungal creams like terbinafine applied for 2-4 weeks.\",\n",
        "        \"Acne vulgaris occurs when hair follicles become clogged. Treatment options include topical retinoids, benzoyl peroxide, and oral antibiotics for severe cases.\",\n",
        "        \"Contact dermatitis results from skin exposure to irritants or allergens. Management involves identifying and avoiding triggers.\",\n",
        "        \"Rosacea causes facial redness and visible blood vessels. Treatment includes avoiding triggers and topical medications like metronidazole.\",\n",
        "        \"Seborrheic dermatitis causes scaly patches on the scalp. Treatment includes medicated shampoos containing ketoconazole.\",\n",
        "        \"Vitiligo causes loss of skin pigmentation. Management includes sun protection, topical corticosteroids, and phototherapy.\",\n",
        "    ]\n",
        "    documents = [\n",
        "        Document(page_content=text, metadata={\"doc_id\": i, \"source\": \"sample_corpus\", \"length\": len(text)})\n",
        "        for i, text in enumerate(sample_texts)\n",
        "    ]\n",
        "    print(f\"‚úÖ Created {len(documents)} sample documents\")\n",
        "\n",
        "if len(documents) > 0:\n",
        "    avg_len = sum(len(d.page_content) for d in documents) // len(documents)\n",
        "    print(f\"  Average length: {avg_len} chars\")\n",
        "    print(f\"  Topics: eczema, psoriasis, fungal infections, treatments\")\n",
        "    print(f\"\\nSample: {documents[0].page_content[:120]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Dense Retrieval (ChromaDB)\n",
        "\n",
        "Build a vector store using dense embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîµ Building dense vector store...\n",
            "‚úÖ Dense index built: 34 docs\n",
            "\n",
            "üîç Dense search: 'What are treatments for eczema?'\n",
            "  [1] Eczema (atopic dermatitis) ‚Ä¢ Core remedies: daily emollients, short lukewarm baths/showers, fragranc...\n",
            "  [2] Eczema (atopic dermatitis) ‚Ä¢ Core remedies: daily emollients, short lukewarm baths/showers, fragranc...\n",
            "  [3] Treatment depends on the underlying cause and may include: ‚Ä¢ Topical medications: Corticosteroids, a...\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "chroma_root = Path(config[\"artifacts_root\"]) / \"chroma\"\n",
        "chroma_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"üîµ Building dense vector store...\")\n",
        "\n",
        "dense_vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"advanced_dense\",\n",
        "    persist_directory=str(chroma_root / \"advanced_dense\"),\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dense index built: {len(documents)} docs\")\n",
        "\n",
        "# Test dense retrieval\n",
        "query = \"What are treatments for eczema?\"\n",
        "dense_results = dense_vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"\\nüîç Dense search: '{query}'\")\n",
        "for i, doc in enumerate(dense_results, 1):\n",
        "    print(f\"  [{i}] {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Sparse Retrieval (BM25)\n",
        "\n",
        "Use BM25 for keyword-based retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üü† Building BM25 index...\n",
            "‚úÖ BM25 index built\n",
            "\n",
            "üîç BM25 search: 'What are treatments for eczema?'\n",
            "  [1] (score: 3.76) Actinic keratoses, BCC, SCC, melanoma ‚Ä¢ What helps: prevention & early detection. Follow the ABCDE s...\n",
            "  [2] (score: 2.61) Urticaria (hives) ‚Ä¢ What helps: for most, second-generation oral antihistamines (non-sedating) are f...\n",
            "  [3] (score: 2.21) 2) Fungal (tinea/ringworm, athlete‚Äôs foot, jock itch) ‚Ä¢ What helps at home: OTC antifungals (creams,...\n"
          ]
        }
      ],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "print(\"üü† Building BM25 index...\")\n",
        "\n",
        "# Tokenize corpus\n",
        "tokenized_corpus = [doc.page_content.lower().split() for doc in documents]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "print(f\"‚úÖ BM25 index built\")\n",
        "\n",
        "def bm25_search(query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Search using BM25 (sparse retrieval algorithm).\n",
        "    \n",
        "    Args:\n",
        "        query: Search query string\n",
        "        top_k: Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries with doc, score, and doc_id\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (‚âà 12-15 lines)\n",
        "    # YOUR CODE HERE\n",
        "    # HINTS:\n",
        "    # 1. Tokenize the query: query.lower().split()\n",
        "    # 2. Get BM25 scores for all documents: bm25.get_scores(tokenized_query)\n",
        "    # 3. Find top-k indices using np.argsort():\n",
        "    #    - np.argsort(scores) gives indices sorted ascending\n",
        "    #    - [::-1] reverses to descending order\n",
        "    #    - [:top_k] takes first top_k indices\n",
        "    # 4. Create an empty results list\n",
        "    # 5. Loop through top_indices\n",
        "    # 6. For each idx, append a dictionary with:\n",
        "    #    - \"doc\": documents[idx]\n",
        "    #    - \"score\": float(scores[idx])\n",
        "    #    - \"doc_id\": idx\n",
        "    # 7. Return the results list\n",
        "    \n",
        "    raise NotImplementedError(\"Complete the bm25_search function\")\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "# Test BM25\n",
        "bm25_results = bm25_search(query, top_k=3)\n",
        "\n",
        "print(f\"\\nüîç BM25 search: '{query}'\")\n",
        "for i, res in enumerate(bm25_results, 1):\n",
        "    print(f\"  [{i}] (score: {res['score']:.2f}) {res['doc'].page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Hybrid Fusion (Dense + BM25)\n",
        "\n",
        "Combine dense and sparse retrieval using Reciprocal Rank Fusion (RRF).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÄ Hybrid (RRF) search: 'What are treatments for eczema?'\n",
            "  [1] (RRF: 0.033) Eczema (atopic dermatitis) ‚Ä¢ Core remedies: daily emollients, short lukewarm baths/showers, fragranc...\n",
            "  [2] (RRF: 0.016) Actinic keratoses, BCC, SCC, melanoma ‚Ä¢ What helps: prevention & early detection. Follow the ABCDE s...\n",
            "  [3] (RRF: 0.016) Urticaria (hives) ‚Ä¢ What helps: for most, second-generation oral antihistamines (non-sedating) are f...\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "def rrf_fusion(dense_docs: List, bm25_results: List, k: int = 60) -> List:\n",
        "    \"\"\"\n",
        "    Reciprocal Rank Fusion - combines dense and sparse retrieval.\n",
        "    \n",
        "    Args:\n",
        "        dense_docs: Results from dense (vector) retrieval\n",
        "        bm25_results: Results from BM25 (sparse) retrieval\n",
        "        k: Constant for RRF formula (default 60)\n",
        "        \n",
        "    Returns:\n",
        "        Fused results sorted by RRF score\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (‚âà 20-25 lines)\n",
        "    # YOUR CODE HERE\n",
        "    # HINTS:\n",
        "    # 1. Initialize an empty dictionary: rrf_scores = {}\n",
        "    #\n",
        "    # 2. Add dense scores:\n",
        "    #    - Loop through dense_docs with enumerate(dense_docs, 1) to get rank and doc\n",
        "    #    - Get doc_id from doc.metadata[\"doc_id\"]\n",
        "    #    - Calculate RRF score: 1.0 / (k + rank)\n",
        "    #    - Add to rrf_scores[doc_id] (initialize to 0.0 if not exists)\n",
        "    #    - Use .get(doc_id, 0.0) to handle missing keys\n",
        "    #\n",
        "    # 3. Add BM25 scores:\n",
        "    #    - Loop through bm25_results with enumerate(bm25_results, 1) to get rank and res\n",
        "    #    - Get doc_id from res[\"doc_id\"]\n",
        "    #    - Calculate RRF score: 1.0 / (k + rank)\n",
        "    #    - Add to rrf_scores[doc_id]\n",
        "    #\n",
        "    # 4. Sort by RRF score:\n",
        "    #    - Use sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    #    - This gives list of (doc_id, score) tuples sorted by score\n",
        "    #\n",
        "    # 5. Build results list:\n",
        "    #    - Loop through sorted_ids\n",
        "    #    - For each (doc_id, score), append dictionary with:\n",
        "    #      - \"doc\": documents[doc_id]\n",
        "    #      - \"score\": score\n",
        "    #      - \"doc_id\": doc_id\n",
        "    #\n",
        "    # 6. Return fused_docs list\n",
        "    \n",
        "    raise NotImplementedError(\"Complete the rrf_fusion function\")\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "# Test hybrid fusion\n",
        "fused_results = rrf_fusion(dense_results, bm25_results)[:3]\n",
        "\n",
        "print(f\"üîÄ Hybrid (RRF) search: '{query}'\")\n",
        "for i, res in enumerate(fused_results, 1):\n",
        "    print(f\"  [{i}] (RRF: {res['score']:.3f}) {res['doc'].page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Reranking with Cross-Encoder\n",
        "\n",
        "Refine results using a cross-encoder for more accurate relevance scoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèÜ Reranked results: 'What are treatments for eczema?'\n",
            "  [1] (rerank: 4.700) Eczema (atopic dermatitis) ‚Ä¢ Core remedies: daily emollients, short lukewarm baths/showers, fragranc...\n",
            "  [2] (rerank: -1.724) Actinic keratoses, BCC, SCC, melanoma ‚Ä¢ What helps: prevention & early detection. Follow the ABCDE s...\n",
            "  [3] (rerank: -4.781) Urticaria (hives) ‚Ä¢ What helps: for most, second-generation oral antihistamines (non-sedating) are f...\n"
          ]
        }
      ],
      "source": [
        "def rerank(query: str, results: List, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Rerank results using a cross-encoder for more accurate relevance scoring.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query\n",
        "        results: List of initial results to rerank\n",
        "        top_k: Number of top results to return after reranking\n",
        "        \n",
        "    Returns:\n",
        "        Reranked results with rerank_score added\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (‚âà 12-15 lines)\n",
        "    # YOUR CODE HERE\n",
        "    # HINTS:\n",
        "    # 1. Prepare query-document pairs for the cross-encoder:\n",
        "    #    - Create list of [query, document_content] pairs\n",
        "    #    - Use list comprehension: [[query, res[\"doc\"].page_content] for res in results]\n",
        "    #    - Store in variable: pairs\n",
        "    #\n",
        "    # 2. Get scores from the cross-encoder:\n",
        "    #    - Use: reranker.predict(pairs)\n",
        "    #    - This returns array of relevance scores\n",
        "    #\n",
        "    # 3. Add rerank scores to results:\n",
        "    #    - Loop through results with enumerate to get index i and result res\n",
        "    #    - Add new field: res[\"rerank_score\"] = float(scores[i])\n",
        "    #\n",
        "    # 4. Sort by rerank_score:\n",
        "    #    - Use: sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "    #    - Take top_k: [:top_k]\n",
        "    #\n",
        "    # 5. Return the reranked results\n",
        "    \n",
        "    raise NotImplementedError(\"Complete the rerank function\")\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "# Test reranking\n",
        "reranked_results = rerank(query, fused_results[:6], top_k=3)\n",
        "\n",
        "print(f\"üèÜ Reranked results: '{query}'\")\n",
        "for i, res in enumerate(reranked_results, 1):\n",
        "    print(f\"  [{i}] (rerank: {res['rerank_score']:.3f}) {res['doc'].page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Bonus Exercise: Complete Hybrid RAG Pipeline\n",
        "\n",
        "**Challenge:** Combine all techniques into a single end-to-end pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_rag_pipeline(query: str, dense_top_n: int = 10, bm25_top_n: int = 10, rerank_top_n: int = 6, final_top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Complete hybrid RAG pipeline combining all advanced techniques.\n",
        "    \n",
        "    Pipeline: Dense Retrieval ‚Üí BM25 Retrieval ‚Üí RRF Fusion ‚Üí Reranking ‚Üí LLM Generation\n",
        "    \n",
        "    Args:\n",
        "        query: User question\n",
        "        dense_top_n: Number of results from dense retrieval\n",
        "        bm25_top_n: Number of results from BM25\n",
        "        rerank_top_n: Number of fused results to rerank\n",
        "        final_top_k: Final number of chunks to use for generation\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with query, answer, and retrieved_docs\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (‚âà 25-30 lines)\n",
        "    # YOUR CODE HERE\n",
        "    # HINTS:\n",
        "    # 1. Dense retrieval:\n",
        "    #    - Use: dense_vectorstore.similarity_search(query, k=dense_top_n)\n",
        "    #    - Store in: dense_results\n",
        "    #\n",
        "    # 2. Sparse (BM25) retrieval:\n",
        "    #    - Use: bm25_search(query, top_k=bm25_top_n)\n",
        "    #    - Store in: bm25_results\n",
        "    #\n",
        "    # 3. Fusion:\n",
        "    #    - Use: rrf_fusion(dense_results, bm25_results)\n",
        "    #    - Take top rerank_top_n: [:rerank_top_n]\n",
        "    #    - Store in: fused_results\n",
        "    #\n",
        "    # 4. Reranking:\n",
        "    #    - Use: rerank(query, fused_results, top_k=final_top_k)\n",
        "    #    - Store in: reranked_results\n",
        "    #\n",
        "    # 5. Build context from reranked results:\n",
        "    #    - Extract text from each result: res[\"doc\"].page_content\n",
        "    #    - Join with \"\\n\\n\"\n",
        "    #    - Store in: context\n",
        "    #\n",
        "    # 6. Build RAG prompt:\n",
        "    #    - System instruction + context + question\n",
        "    #    - Example: f\"\"\"Use the context to answer the question...\n",
        "    #                  Context: {context}\n",
        "    #                  Question: {query}\n",
        "    #                  Answer:\"\"\"\n",
        "    #\n",
        "    # 7. Generate answer:\n",
        "    #    - Use: llm.invoke(prompt)\n",
        "    #    - Extract text: response.content if hasattr(response, 'content') else str(response)\n",
        "    #\n",
        "    # 8. Return dictionary with:\n",
        "    #    - \"query\": query\n",
        "    #    - \"answer\": answer\n",
        "    #    - \"retrieved_docs\": reranked_results\n",
        "    #    - \"num_dense\": len(dense_results)\n",
        "    #    - \"num_bm25\": len(bm25_results)\n",
        "    #    - \"num_fused\": len(fused_results)\n",
        "    #    - \"num_final\": len(reranked_results)\n",
        "    \n",
        "    raise NotImplementedError(\"Complete the hybrid_rag_pipeline function\")\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "# Test the complete pipeline\n",
        "print(\"üöÄ Testing Complete Hybrid RAG Pipeline\\n\")\n",
        "test_query = \"What are treatments for eczema?\"\n",
        "result = hybrid_rag_pipeline(test_query)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"\\nPipeline stats:\")\n",
        "print(f\"  Dense retrieval: {result.get('num_dense', 0)} docs\")\n",
        "print(f\"  BM25 retrieval: {result.get('num_bm25', 0)} docs\")\n",
        "print(f\"  After fusion: {result.get('num_fused', 0)} docs\")\n",
        "print(f\"  After reranking: {result.get('num_final', 0)} docs\")\n",
        "print(f\"\\nFinal Answer:\\n{result['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**What we covered:**\n",
        "\n",
        "‚úÖ **Dense Retrieval** - Vector similarity search with ChromaDB  \n",
        "‚úÖ **Sparse Retrieval** - BM25 keyword matching  \n",
        "‚úÖ **Hybrid Fusion** - Reciprocal Rank Fusion (RRF) combining both  \n",
        "‚úÖ **Reranking** - Cross-encoder for refined relevance scoring  \n",
        "‚úÖ **Dynamic Loading** - Corpus loaded from raw_text files  \n",
        "\n",
        "**Retrieval Pipeline:**\n",
        "```\n",
        "Query ‚Üí Dense (top-N) + BM25 (top-N) ‚Üí Fusion (RRF) ‚Üí Rerank (cross-encoder) ‚Üí Final top-k\n",
        "```\n",
        "\n",
        "**When to use each:**\n",
        "- **BM25**: Keyword/exact match queries\n",
        "- **Dense**: Semantic/paraphrase queries  \n",
        "- **Hybrid**: Best of both worlds\n",
        "- **Reranking**: Highest precision (slower but more accurate)\n",
        "\n",
        "**Trade-offs:**\n",
        "- Hybrid + reranking: Better accuracy, higher latency\n",
        "- Dense only: Fast, good for semantic search\n",
        "- BM25 only: Fast, good for keyword search\n",
        "\n",
        "**Artifacts:**\n",
        "- `./artifacts/chroma/advanced_dense/`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
