{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 – LangChain PDF RAG with Chunking Strategies\n",
    "\n",
    "**Learning Goals:**\n",
    "- Use LangChain's document loaders and text splitters\n",
    "- Persist embeddings in ChromaDB\n",
    "- Compare chunking strategies: recursive, fixed, sentence-based\n",
    "- Build a complete RAG pipeline with LangChain\n",
    "\n",
    "**What we'll build:**\n",
    "1. Load PDFs using LangChain loaders\n",
    "2. Test three chunking strategies (RecursiveCharacterTextSplitter, CharacterTextSplitter, SentenceSplitter)\n",
    "3. Embed chunks using configurable models (OpenAI/Cohere/SBERT)\n",
    "4. Store in ChromaDB with metadata\n",
    "5. Query with retrieval + QA chain\n",
    "\n",
    "**Persistence:**\n",
    "- `./artifacts/chroma/langchain_recursive/`\n",
    "- `./artifacts/chroma/langchain_fixed/`\n",
    "- `./artifacts/chroma/langchain_sentence/`\n",
    "- `./artifacts/manifests/langchain_{strategy}.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Working directory: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Week 03\n",
      "✅ Config loaded:\n",
      "  LLM: openai / gpt-4o-mini\n",
      "  Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
      "  Temperature: 0.2\n",
      "  Artifacts: ./artifacts\n"
     ]
    }
   ],
   "source": [
    "#  Global Config & Services (using centralized modules)\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path and change to project root\n",
    "import os\n",
    "\n",
    "# Get the notebook's current directory and find project root\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == \"notebooks\":\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "# Change to project root and add to path\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\" Working directory: {os.getcwd()}\")\n",
    "\n",
    "from src.services.llm_services import (\n",
    "    load_config,\n",
    "    get_llm,\n",
    "    get_text_embeddings,\n",
    "    validate_api_keys,\n",
    "    print_config_summary\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load configuration from config.yaml (now we're in project root)\n",
    "config = load_config(\"src/config/config.yaml\")\n",
    "\n",
    "# Validate API keys\n",
    "validate_api_keys(config, verbose=True)\n",
    "\n",
    "# Print summary\n",
    "print_config_summary(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Week 03\\src\\services\\llm_services.py:129: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLM: openai / gpt-4o-mini\n",
      " Embeddings: sbert / sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      " Testing LLM API connection...\n",
      " LLM API verified: API working!\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "# Initialize LLM and Embeddings using factories from llm_services\n",
    "llm = get_llm(config)\n",
    "embeddings = get_text_embeddings(config)\n",
    "\n",
    "print(f\" LLM: {config['llm_provider']} / {config['llm_model']}\")\n",
    "print(f\" Embeddings: {config['text_emb_provider']} / {config['text_emb_model']}\")\n",
    "\n",
    "# Verify API key with test completion\n",
    "print(\"\\n Testing LLM API connection...\")\n",
    "try:\n",
    "    test_response = llm.invoke(\"Say 'API working!' if you can read this.\")\n",
    "    test_msg = test_response.content if hasattr(test_response, 'content') else str(test_response)\n",
    "    print(f\" LLM API verified: {test_msg[:50]}\")\n",
    "except Exception as e:\n",
    "    print(f\" LLM API test failed: {e}\")\n",
    "    print(\"  Please check your .env file and API key configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 198 document pages\n",
      "  Total characters: 127,894\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "                                                PyPDFLoader, \n",
    "                                                DirectoryLoader, \n",
    "                                                TextLoader\n",
    "                                                )\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "pdf_dir = Path(config[\"data_root\"]) / \"pdfs\"\n",
    "pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Try loading PDFs\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    print(\"  No PDFs found. Creating sample text document...\")\n",
    "    \n",
    "    sample_content = \"\"\"# Common Skin Diseases and Conditions\n",
    "\n",
    "Understanding skin diseases is essential for proper care and treatment. The skin is the largest organ and serves as a protective barrier.\n",
    "\n",
    "## Inflammatory Skin Conditions\n",
    "\n",
    "### Eczema (Atopic Dermatitis)\n",
    "Eczema is a chronic inflammatory condition marked by itchy, dry, and red skin. It affects 10-20% of children and often has a genetic component. Treatment includes daily moisturizing, avoiding triggers, and topical anti-inflammatory medications.\n",
    "\n",
    "### Psoriasis\n",
    "Psoriasis is an autoimmune condition causing thick, silvery scales and red plaques. Treatment options include topical corticosteroids, phototherapy, and systemic medications for moderate to severe cases.\n",
    "\n",
    "## Fungal Infections\n",
    "\n",
    "### Ringworm (Tinea)\n",
    "Fungal infections cause circular, red, scaly patches. Common types include athlete's foot (tinea pedis) and jock itch (tinea cruris). Treatment involves topical or oral antifungal medications.\n",
    "\n",
    "### Treatment Approach\n",
    "Keep affected areas dry and clean. Use antifungal creams like terbinafine for 2-4 weeks as directed.\n",
    "\n",
    "## Bacterial Infections\n",
    "\n",
    "### Impetigo\n",
    "A common bacterial infection in children causing honey-colored crusts. Requires prompt medical assessment and antibiotic treatment.\n",
    "\n",
    "### Cellulitis\n",
    "A deep bacterial infection causing swelling, redness, and warmth. Requires oral or IV antibiotics.\n",
    "\n",
    "## General Skin Care\n",
    "\n",
    "Daily moisturizing with thick creams helps maintain the skin barrier. Use broad-spectrum SPF 30+ sunscreen to protect against UV damage. Identify and avoid personal triggers like harsh detergents, allergens, and stress.\n",
    "\"\"\"\n",
    "    \n",
    "    sample_file = pdf_dir / \"skin_diseases_intro.txt\"\n",
    "    sample_file.write_text(sample_content)\n",
    "    \n",
    "    # Load text file as document\n",
    "    documents = [Document(page_content=sample_content, metadata={\"source\": \"skin_diseases_intro.txt\", \"page\": 0})]\n",
    "    \n",
    "else:\n",
    "    # Load PDFs\n",
    "    documents = []\n",
    "    for pdf_path in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)\n",
    "\n",
    "print(f\"✅ Loaded {len(documents)} document pages\")\n",
    "print(f\"  Total characters: {sum(len(d.page_content) for d in documents):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Chunking Strategies\n",
    "\n",
    "We'll implement three chunking strategies:\n",
    "\n",
    "1. **Recursive** - Splits by paragraphs, then sentences, then characters\n",
    "2. **Fixed** - Fixed character length with overlap\n",
    "3. **Sentence** - Splits on sentence boundaries (using langchain-experimental)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ recursive :  280 chunks\n",
      "✅ fixed     :  275 chunks\n",
      "\n",
      "Example chunk (recursive):\n",
      "  Length: 183 chars\n",
      "  Content: Common Skin Disease\n",
      "Surangkana Veeranawin, MD\n",
      "MSc, Queen Mary University of London\n",
      "Mmed, University of Sydney\n",
      "DipDerm RCPS(Glas.)\n",
      "Diplomate American B...\n",
      "  Metadata: {'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2019-05-29T13:06:19+07:00', 'title': 'Common Skin Disease', 'author': 'Surangkana Veeranawin', 'moddate': '2019-05-29T13:06:19+07:00', 'source': 'data\\\\pdfs\\\\3-Lecture-Common-Skin-Disease-2019.pdf', 'total_pages': 119, 'page': 0, 'page_label': '1', 'splitter': 'recursive'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "\n",
    "def get_splitter(strategy: str, chunk_size: int = 800, chunk_overlap: int = 150):\n",
    "    \"\"\"\n",
    "    Return a text splitter based on the specified strategy.\n",
    "    \n",
    "    Args:\n",
    "        strategy: One of \"recursive\", \"fixed\", or \"sentence\"\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        A LangChain text splitter instance\n",
    "    \"\"\"\n",
    "    if strategy == \"recursive\": \n",
    "        return RecursiveCharacterTextSplitter(\n",
    "                                            chunk_size=chunk_size,\n",
    "                                            chunk_overlap=chunk_overlap,\n",
    "                                            separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]\n",
    "                                            )\n",
    "\n",
    "    elif strategy == \"fixed\":\n",
    "        return CharacterTextSplitter(\n",
    "                                    chunk_size=chunk_size,\n",
    "                                    chunk_overlap=chunk_overlap,\n",
    "                                    separator=\" \"\n",
    "                                    )\n",
    "\n",
    "    elif strategy == \"sentence\":\n",
    "        pass \n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown stratergy\")\n",
    "\n",
    "\n",
    "# Test all strategies\n",
    "# strategies = [\"recursive\", \"fixed\", \"sentence\"]\n",
    "strategies = [\"recursive\", \"fixed\"]\n",
    "split_results = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    splitter = get_splitter(strategy)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    # Add strategy to metadata\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"splitter\"] = strategy\n",
    "    \n",
    "    split_results[strategy] = chunks\n",
    "    print(f\"✅ {strategy:10s}: {len(chunks):4d} chunks\")\n",
    "\n",
    "print(f\"\\nExample chunk (recursive):\")\n",
    "print(f\"  Length: {len(split_results['recursive'][0].page_content)} chars\")\n",
    "print(f\"  Content: {split_results['recursive'][0].page_content[:150]}...\")\n",
    "print(f\"  Metadata: {split_results['recursive'][0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Build ChromaDB Collections\n",
    "\n",
    "For each chunking strategy, we'll create a separate ChromaDB collection with embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building collection: langchain_recursive...\n",
      "  ✅ Persisted to artifacts\\chroma\\langchain_recursive\n",
      "  ✅ 280 chunks embedded\n",
      "Building collection: langchain_fixed...\n",
      "  ✅ Persisted to artifacts\\chroma\\langchain_fixed\n",
      "  ✅ 275 chunks embedded\n",
      "\n",
      "✅ All collections built!\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_root = Path(config[\"artifacts_root\"]) / \"chroma\"\n",
    "chroma_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vectorstores = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    collection_name = f\"langchain_{strategy}\"\n",
    "    persist_dir = str(chroma_root / collection_name)\n",
    "    \n",
    "    print(f\"Building collection: {collection_name}...\")\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "                                        documents=split_results[strategy],\n",
    "                                        embedding=embeddings,\n",
    "                                        collection_name=collection_name,\n",
    "                                        persist_directory=persist_dir\n",
    "                                        )\n",
    "    \n",
    "    print(f\"  ✅ Persisted to {persist_dir}\")\n",
    "    print(f\"  ✅ {len(split_results[strategy])} chunks embedded\")\n",
    "\n",
    "    vectorstores[strategy] = vectorstore\n",
    "\n",
    "print(f\"\\n✅ All collections built!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Manifests\n",
    "\n",
    "Each collection gets a manifest tracking build parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Manifest saved: langchain_recursive.json\n",
      "✅ Manifest saved: langchain_fixed.json\n"
     ]
    }
   ],
   "source": [
    "manifests_dir = Path(config[\"artifacts_root\"]) / \"manifests\"\n",
    "manifests_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for strategy in strategies:\n",
    "    manifest = {\n",
    "        \"collection_name\": f\"langchain_{strategy}\",\n",
    "        \"framework\": \"langchain\",\n",
    "        \"strategy\": strategy,\n",
    "        \"embedding_model\": config[\"text_emb_model\"],\n",
    "        \"embedding_provider\": config[\"text_emb_provider\"],\n",
    "        \"normalize\": config[\"normalize_embeddings\"],\n",
    "        \"chunk_size\": 800,\n",
    "        \"chunk_overlap\": 150,\n",
    "        \"num_chunks\": len(split_results[strategy]),\n",
    "        \"num_documents\": len(documents),\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    manifest_path = manifests_dir / f\"langchain_{strategy}.json\"\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Manifest saved: {manifest_path.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Retrieval + QA Chain\n",
    "\n",
    "We'll build a simple RAG chain using LangChain's stuff chain (concatenate all retrieved chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QA chains ready for all strategies\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG prompt template\n",
    "rag_prompt_template = \"\"\"You are a concise assitant for healthcare. Use only the provided context to answer the question.\n",
    "Keep answers under 5 sentences. Based on below conditions adapt your answer.\n",
    "\n",
    "1. If question is about Skin diseases and the infomation is insufficent, say \"I do not have info, please reach to our hospital\".\n",
    "2. If question is iirelevant to Skin diseases, say \"I can't provide info\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "RAG_PROMPT = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def build_qa_chain(strategy: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Build a RAG chain for a given chunking strategy using LangChain LCEL.\n",
    "    \n",
    "    Args:\n",
    "        strategy: The chunking strategy name (e.g., \"recursive\", \"fixed\", \"sentence\")\n",
    "        top_k: Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        A RAG chain ready to answer questions\n",
    "    \"\"\"\n",
    "    vectorstore = vectorstores[strategy]\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\" : top_k})\n",
    "    \n",
    "    # Build RAG chain using LCEL\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Chain that takes a string question and returns formatted answer\n",
    "    qa_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | RAG_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Create a wrapper that also returns source documents for comparison\n",
    "    class RAGChainWithSources:\n",
    "        def __init__(self, chain, retriever):\n",
    "            self.chain = chain\n",
    "            self.retriever = retriever\n",
    "        \n",
    "        def invoke(self, query_dict):\n",
    "            # Extract query from dict\n",
    "            query = query_dict.get(\"query\", query_dict) if isinstance(query_dict, dict) else query_dict\n",
    "            # Invoke chain with query string\n",
    "            result = self.chain.invoke(query)\n",
    "            # Get source documents\n",
    "            source_docs = self.retriever.invoke(query)\n",
    "            return {\"result\": result, \"source_documents\": source_docs}\n",
    "    \n",
    "    return RAGChainWithSources(qa_chain, retriever)\n",
    "    \n",
    "# Build chains for all strategies\n",
    "qa_chains = {strategy: build_qa_chain(strategy) for strategy in strategies}\n",
    "\n",
    "print(\" QA chains ready for all strategies\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interactive Demo: Compare Chunking Strategies\n",
    "\n",
    "Choose a splitter and ask questions to see how different chunking approaches affect retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing strategy: recursive\n",
      "\n",
      "================================================================================\n",
      "\n",
      " Query: What causes eczema and atopic dermatitis?\n",
      "\n",
      " Retrieved chunks:\n",
      "  [1] data\\pdfs\\Derm_Handbook_3rd-Edition-_Nov_2020-FINAL.pdf (page 47)\n",
      "      Dermatology: Handbook for medical students & junior doctors  \n",
      "   British Association of Dermatologists 47 \n",
      "Atopic eczema...\n",
      "\n",
      "  [2] data\\pdfs\\3-Lecture-Common-Skin-Disease-2019.pdf (page 109)\n",
      "      Atopic Dermatitis\n",
      " Chronic inflammatory dermatosis affecting 10%-20% of \n",
      "children (esp. infants and young children)\n",
      " E...\n",
      "\n",
      "  [3] data\\pdfs\\3-Lecture-Common-Skin-Disease-2019.pdf (page 116)\n",
      "      Atopic Dermatitis\n",
      " Minor, less specific feature: \n",
      "- xerosis, ichthyosis, palmar hyperlinearity, keratosis pilaris. \n",
      "Imm...\n",
      "\n",
      " Answer:\n",
      "Eczema and atopic dermatitis are caused by a combination of factors that are not fully understood. A positive family history of atopy (eczema, asthma, allergic rhinitis) is often present, and there may be a primary genetic defect in skin barrier function, specifically involving the protein filaggrin. Exacerbating factors include infections, allergens (such as chemicals, food, dust, and pet fur), sweating, heat, occupation, and severe stress.\n",
      "\n",
      "================================================================================\n",
      "\n",
      " Query: How do you treat fungal infections like ringworm?\n",
      "\n",
      " Retrieved chunks:\n",
      "  [1] data\\pdfs\\Derm_Handbook_3rd-Edition-_Nov_2020-FINAL.pdf (page 40)\n",
      "      Dermatology: Handbook for medical students & junior doctors  \n",
      "   British Association of Dermatologists 40 \n",
      "● Topical ant...\n",
      "\n",
      "  [2] data\\pdfs\\Derm_Handbook_3rd-Edition-_Nov_2020-FINAL.pdf (page 39)\n",
      "      edge is typical \n",
      "● Tinea cruris (tinea infection of the groin and natal cleft) – very     \n",
      "    itchy, similar to tinea c...\n",
      "\n",
      "  [3] data\\pdfs\\Derm_Handbook_3rd-Edition-_Nov_2020-FINAL.pdf (page 39)\n",
      "      Dermatology: Handbook for medical students & junior doctors  \n",
      "   British Association of Dermatologists 39 \n",
      "Superficial f...\n",
      "\n",
      " Answer:\n",
      "For treating fungal infections like ringworm, topical antifungal agents such as terbinafine cream are commonly used. In cases of severe, widespread, or nail infections, oral antifungal agents like itraconazole may be necessary. It's important to avoid topical steroids, as they can lead to tinea incognito. Additionally, addressing any predisposing factors, such as a moist environment or underlying immunosuppression, is recommended.\n",
      "\n",
      "================================================================================\n",
      "\n",
      " Query: What are the recommended treatments for psoriasis?\n",
      "\n",
      " Retrieved chunks:\n",
      "  [1] data\\pdfs\\3-Lecture-Common-Skin-Disease-2019.pdf (page 42)\n",
      "      Psoriasis\n",
      " Scalp; 10% salicylic acid in mineral oil qhs, tar shampoo \n",
      "qd, steroid lotion, calcipotriol lotion.\n",
      " Nails;...\n",
      "\n",
      "  [2] data\\pdfs\\3-Lecture-Common-Skin-Disease-2019.pdf (page 36)\n",
      "      Psoriasis (Inverse type)...\n",
      "\n",
      "  [3] data\\pdfs\\3-Lecture-Common-Skin-Disease-2019.pdf (page 35)\n",
      "      Psoriasis (Palmoplantar)...\n",
      "\n",
      " Answer:\n",
      "For psoriasis, treatments include a 10% salicylic acid in mineral oil for the scalp, tar shampoo, steroid lotion, and calcipotriol lotion. For nail psoriasis, intradermal triamcinolone acetonide injections, methotrexate, and new biologics are recommended. Generalized psoriasis (>10% BSA) benefits from systemic therapy, which should be administered by an experienced physician.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example queries about dermatology\n",
    "test_queries = [\n",
    "    \"What causes eczema and atopic dermatitis?\",\n",
    "    \"How do you treat fungal infections like ringworm?\",\n",
    "    \"What are the recommended treatments for psoriasis?\",\n",
    "]\n",
    "\n",
    "selected_strategy = \"recursive\"  # Change to \"fixed\" or \"sentence\" to compare\n",
    "\n",
    "print(f\" Testing strategy: {selected_strategy}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n Query: {query}\\n\")\n",
    "    \n",
    "    qa_chain = qa_chains[selected_strategy]\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    \n",
    "    print(\" Retrieved chunks:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        print(f\"  [{i}] {source} (page {page})\")\n",
    "        print(f\"      {doc.page_content[:120]}...\\n\")\n",
    "    \n",
    "    print(f\" Answer:\\n{result['result']}\\n\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "- ✅ LangChain provides flexible document loaders and text splitters\n",
    "- ✅ Chunking strategy affects retrieval quality\n",
    "- ✅ ChromaDB persists embeddings efficiently\n",
    "- ✅ RetrievalQA chain simplifies RAG pipelines\n",
    "\n",
    "**Chunking trade-offs:**\n",
    "- **Recursive**: Respects document structure (paragraphs, sentences)\n",
    "- **Fixed**: Simple, predictable chunk sizes\n",
    "- **Sentence**: Preserves semantic boundaries but variable lengths\n",
    "\n",
    "**Rebuild triggers:**\n",
    "- Change embedding model/normalization → rebuild collections\n",
    "- Change chunking strategy → new collection name\n",
    "- Change dataset → re-embed and rebuild\n",
    "\n",
    "**Artifacts:**\n",
    "- `./artifacts/chroma/langchain_recursive/`\n",
    "- `./artifacts/chroma/langchain_fixed/`\n",
    "- `./artifacts/chroma/langchain_sentence/`\n",
    "- `./artifacts/manifests/langchain_{strategy}.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sahas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
